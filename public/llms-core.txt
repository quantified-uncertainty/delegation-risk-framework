# Delegation Risk Framework - Core Documentation

> Version: 1.0.2 | Generated: 2025-12-23
>
> Essential pages for understanding the framework.
> For complete documentation, see: https://delegation-risk.org

================================================================================


------------------------------------------------------------
## Getting Started
Source: getting-started/index.md
------------------------------------------------------------

This section helps you understand Delegation Risk at whatever depth you need.

## Choose Your Path

### Quick Understanding (5-10 minutes)
If you want the core ideas fast:
1. **[Five-Minute Intro](/getting-started/five-minute-intro/)** — The essential concepts in one page

### Solid Foundation (30-60 minutes)
If you're evaluating whether this framework is useful:
1. **[Introduction](/getting-started/introduction/)** — The problem we're solving and our approach
2. **[Core Concepts](/getting-started/core-concepts/)** — Key ideas with visual framework diagram
3. **[FAQ](/getting-started/faq/)** — Common objections answered

### Ready to Apply (2-3 hours)
If you want to use this framework on a real system:
1. **[Introduction](/getting-started/introduction/)** — Problem context
2. **[Core Concepts](/getting-started/core-concepts/)** — The conceptual foundation
3. **[Delegation Risk](/delegation-risk/)** — Quantitative details
4. **[Quick Start](/design-patterns/tools/quick-start/)** — Step-by-step application checklist

## Section Contents

| Page | Purpose | Reading Time |
|------|---------|--------------|
| [Introduction](/getting-started/introduction/) | Why delegation risk matters, the core problem | 10 min |
| [Five-Minute Intro](/getting-started/five-minute-intro/) | Compressed overview for busy readers | 5 min |
| [Core Concepts](/getting-started/core-concepts/) | Visual framework, key ideas without math | 20 min |
| [FAQ](/getting-started/faq/) | Objections, edge cases, clarifications | 15 min |
| [Glossary](/getting-started/glossary/) | Term definitions (reference) | — |

## How This Section Connects


[Diagram: flowchart TB]


## Next Steps

Once you understand the basics:
- **Want the math?** → [Delegation Risk](/delegation-risk/) for quantitative foundations
- **Want to apply it?** → [Design Patterns](/design-patterns/) for practical guidance
- **Want examples?** → [Case Studies](/case-studies/) for real-world applications


------------------------------------------------------------
## 5-Minute Introduction
Source: getting-started/five-minute-intro.md
------------------------------------------------------------

# Delegation Risk in 5 Minutes


> **NOTE**
>
> This is the compressed version. For caveats and nuance, see the [full Introduction](/getting-started/introduction/).


**The problem**: AI agents can cause real harm. One powerful agent with full access is risky—if it fails or schemes, damage may be unbounded.

**The proposed solution**: Decompose into many limited components. Budget risk like money.

---

## The Core Idea

Instead of this (stylized—real systems exist on a spectrum):

```
User → [Powerful AI Agent] → Any Action
              ↑
        Full context
        Full capability
        Full autonomy
```

Consider this:

```
User → [Router] → [Narrow Component 1] → [Verifier] → Limited Action 1
                → [Narrow Component 2] → [Verifier] → Limited Action 2
                → [Narrow Component 3] → [Human Gate] → Sensitive Action
```

Each component has:
- **Limited capability** (can only do specific things)
- **Limited context** (doesn't see the whole system)
- **Limited autonomy** (verified or human-approved)

---

## The Formula

**Delegation Risk** = probability of bad outcome × damage

```
Delegation Risk = Σ P(failure) × Damage
```

Example: Component has 1% chance of causing $10,000 damage → Delegation Risk = $100

**Budget your Delegation Risk** across components. If total exceeds budget, add safeguards.

---

## The Principles

Apply "Least X" to every component:

| Principle | Meaning |
|-----------|---------|
| **Least Intelligence** | Dumbest implementation that works |
| **Least Privilege** | Minimum permissions |
| **Least Context** | Minimum information |
| **Least Persistence** | No memory between calls |
| **Least Autonomy** | Human oversight where needed |

---

## The Hierarchy

Use the most verifiable option:

```
Best → Verified Code → Regular Code → Narrow Models → General LLMs → Worst
```

Don't use GPT-4 for things a regex can do.

---

## Quick Example

**Task**: Slack bot answering questions from docs

| Component | Implementation | Delegation Risk |
|-----------|---------------|-----|
| Retriever | Code (vector search) | $5/mo |
| Answerer | Fine-tuned 7B | $50/mo |
| Poster | Code (rate limited) | $1/mo |
| **Total** | | **$56/mo** |

Budget: $500/mo → **Within budget. Ship it.**

---

## When to Use This

| Situation | Framework Value |
|-----------|-----------------|
| Internal tools, recoverable mistakes | Light application |
| Customer-facing products | Medium rigor |
| Autonomous agents with real-world actions | Full application |
| Safety-critical systems | Essential |

---

## Next Steps

- **Want the full picture?** → [Core Concepts](/getting-started/core-concepts/)
- **Ready to apply it?** → [Quick Start](/design-patterns/tools/quick-start/)
- **Want the math?** → [Delegation Risk](/delegation-risk/overview/)
- **Have questions?** → [FAQ](/getting-started/faq/)

---

**TL;DR**: Consider building many limited components instead of one powerful AI. Quantify risk. Budget it like money.


------------------------------------------------------------
## Core Concepts
Source: getting-started/core-concepts.md
------------------------------------------------------------

# Core Concepts

This page introduces the key ideas without mathematical formalism. For quantitative details, see [Delegation Risk](/delegation-risk/overview/).

## Framework Overview

How all the pieces fit together:


[Diagram: flowchart TB]


**Reading the diagram**: Delegation Risk is the foundation. Theory explains how it behaves. Methods provide established approaches from other fields. Principles give actionable constraints. Application shows how to build systems (with AI as primary focus).

## Delegation Risk

When you delegate a task, you're accepting **risk**—the potential for harm if the delegate fails or misbehaves.

**Delegation Risk** quantifies this: for each delegate, sum over all possible bad outcomes, weighted by their probability. A delegate with access to your bank account has higher Delegation Risk than one that can only read public web pages.


> **TIP**
>
> Risk is finite and should be budgeted. Just as organizations allocate compute and money, they should allocate delegation risk—tracking how much they're accepting, from which delegates, for what purposes.


### The Formula

**Delegation Risk = Σ P(harm mode) × Damage(harm mode)**

For each way something could go wrong (harm mode), multiply:
- The probability it happens
- The damage if it does

Sum across all harm modes to get total Delegation Risk.

### Example: Employee with Check-Writing Authority

| Harm Mode | Probability | Damage | Risk Contribution |
|-----------|-------------|--------|-------------------|
| Clerical error | 0.01 | $1,000 | $10 |
| Unauthorized payment | 0.001 | $50,000 | $50 |
| Fraud | 0.0001 | $500,000 | $50 |

**Delegation Risk** = $10 + $50 + $50 = **$110** expected cost

This doesn't mean you'll lose $110. It means that over many such delegations, you'd expect $110 in losses on average. You can compare this to the value the delegation provides and decide if it's worth it.

## Risk Propagation

When delegates delegate to other delegates, risk relationships form networks. If A trusts B with weight 0.8, and B trusts C with weight 0.7, how much should A trust C?

Different rules give different answers:
- **Multiplicative**: 0.8 × 0.7 = 0.56 (each stage is independent risk)
- **Minimum**: min(0.8, 0.7) = 0.7 (chain only as strong as weakest link)
- **Discounted**: More complex models accounting for path length


[Diagram: graph LR]


The right rule depends on what "trust" means in your context. The framework provides formal tools for reasoning about these relationships.


**Choosing a propagation rule**


| Rule | Formula | Best when... |
|------|---------|--------------|
| **Multiplicative** | T(A→C) = T(A→B) × T(B→C) | Each delegation is independent risk; default choice |
| **Minimum** | T(A→C) = min(T(A→B), T(B→C)) | Chain only as strong as weakest link |
| **Discounted** | T(A→C) = T(A→B) × T(B→C) × δⁿ | Longer paths inherently less trustworthy |
| **Maximum path** | T(A→C) = max over all paths | Any valid route suffices |

Most systems should start with **multiplicative** propagation—it's intuitive, conservative, and composes well.


> **Related: Social Choice Theory**
>
> Trust aggregation across multiple delegates relates to [social choice theory](https://en.wikipedia.org/wiki/Social_choice_theory)—how to combine individual inputs into collective outcomes. [Arrow's impossibility theorem](https://en.wikipedia.org/wiki/Arrow%27s_impossibility_theorem) has implications for trust aggregation: no perfect rule exists.


## Decomposition Over Monoliths

A single powerful delegate is dangerous because it concentrates capability, context, and control. If it fails or acts against you, the damage is unbounded.

**Decomposition** breaks this pattern:
- Split tasks across many specialized components
- Each component has limited capability and context
- No single component can cause catastrophic harm
- Components can't easily coordinate against the system

Think of it like [separation of powers](https://en.wikipedia.org/wiki/Separation_of_powers) in government, or [microservices](https://en.wikipedia.org/wiki/Microservices) in software architecture.

### Example: Organizational Analogy

| Aspect | Centralized | Decomposed |
|--------|-------------|------------|
| **Single point of failure** | CEO makes all decisions | Authority distributed across roles |
| **Blast radius** | Bad CEO decision affects everything | Bad decision contained to department |
| **Coordination risk** | CEO can pursue personal agenda | Requires multiple people to collude |
| **Verification** | Must verify CEO's judgment on everything | Verify narrow, specific behaviors |

The same logic applies to software systems and AI agents.

## The "Least X" Principles

Security engineering has the [principle of least privilege](https://en.wikipedia.org/wiki/Principle_of_least_privilege): give each component minimum necessary access. We extend this across multiple dimensions:

- **Least Capability**: Minimum ability to take actions
- **Least Privilege**: Minimum permissions and access rights
- **Least Context**: Minimum information about the broader system
- **Least Persistence**: Minimum state maintained across invocations
- **Least Autonomy**: Minimum ability to act without oversight
- **Least Connectivity**: Minimum communication paths between components

Each principle bounds a different attack surface. Applied together, they create components that are limited in what they can do, know, remember, and coordinate.


**Why each principle matters**


| Principle | What it limits | Risk bounded |
|-----------|---------------|--------------|
| Least Capability | Ability to act | Direct damage from actions |
| Least Privilege | Access rights | Scope of potential harm |
| Least Context | System knowledge | Strategic planning against system |
| Least Persistence | Memory across calls | Long-term planning, accumulating leverage |
| Least Autonomy | Unsupervised action | Scope of damage before intervention |
| Least Connectivity | Communication paths | Coordination between components |


## Risk Budgeting

Borrowed from finance and nuclear safety: set a total acceptable risk level for the system, then allocate risk budgets to components.

Key requirements:
- **Compositional guarantees**: Component risks must aggregate predictably
- **Principled allocation**: Methods like [Euler allocation](/cross-domain-methods/euler-allocation/) ensure budgets sum correctly
- **Incentive-compatible reporting**: Mechanisms that make honest risk reporting optimal
- **Verification infrastructure**: Independent confirmation that claimed levels match reality
- **Conservative margins**: Buffer for uncertainty and unknowns


> **NOTE**
>
> This isn't theoretical—nuclear plants operate at 10⁻⁹ failure probability per reactor-year by flowing system targets down to component budgets through fault trees.


[Diagram: flowchart TD]


## Architectural Safety

The central claim: safety can be a property of system architecture, not just individual component behavior.

If you can't guarantee a delegate is trustworthy, you can still:
- Limit what they can access (least privilege)
- Limit what they know (least context)
- Limit what they can coordinate (decomposition)
- Limit how much damage they can cause (risk budgets)
- Verify their behavior matches claims (verification layers)


> **CAUTION**
>
> These structural constraints provide defense in depth. They don't replace selecting trustworthy delegates—they bound the damage when trust is misplaced.


---

## Application to AI Systems

The framework applies generally, but AI systems are our primary focus. For AI specifically:

### Verifiability Hierarchy

Not all implementations are equally trustworthy:

1. **Provably secure code** — Mathematical guarantees of correctness
2. **Regular code** — Auditable, testable, deterministic
3. **Fine-tuned narrow models** — Predictable within a specific domain
4. **Constrained general models** — LLMs with extensive prompting and evaluation
5. **Base LLMs** — General capability, harder to verify
6. **RL systems** — Learned policies, least predictable


> **Design Principle**
>
> Use the most verifiable implementation that achieves the required functionality. Don't use a frontier LLM for tasks that can be done with code or a narrow model.


[Diagram: graph TB]


**When to use each level**


| Level | Use when... | Example |
|-------|-------------|---------|
| Provably secure code | Correctness is critical, logic is simple | Access control checks, rate limiters |
| Regular code | Behavior is fully specifiable | Data transformation, API routing |
| Fine-tuned narrow models | Task is well-defined, training data exists | Sentiment classification, entity extraction |
| Constrained general models | Judgment needed, outcomes recoverable | Draft generation, summarization |
| Base LLMs | Maximum flexibility needed, heavy oversight | Creative tasks, complex reasoning |
| RL systems | Learning from feedback is essential | Rarely recommended for safety-critical paths |


### Architectural Spectrum: Monolithic vs Decomposed

Real systems exist on a spectrum. This diagram shows two stylized endpoints:


[Diagram: flowchart TB]


| Aspect | Monolithic | Decomposed |
|--------|------------|------------|
| **Single point of failure** | Yes—one bad output, system fails | No—failure isolated to component |
| **Attack surface** | Entire model capability | Only exposed component interfaces |
| **Verification** | Must verify everything the model can do | Verify narrow, specific behaviors |
| **Blast radius** | Unlimited—model can do anything | Bounded—each component has limits |
| **Coordination risk** | Model coordinates with itself | Components can't easily collude |

---

## What This Framework Provides

1. **Vocabulary**: Precise terms for discussing risk, delegation, and containment
2. **Theory**: Mathematics for risk propagation, capability quantification, and optimization
3. **Methods**: Established approaches adapted from finance, nuclear safety, mechanism design
4. **Principles**: Actionable design constraints (the "Least X" family)
5. **Patterns**: Decomposed coordination, verification layers, safety mechanisms

The goal is infrastructure for safely delegating at scale—not a complete solution to trust, but a foundation for managing risk as capabilities increase.

### The Optimization Problem

The framework isn't just about minimizing risk—it's about **maximizing capability subject to risk constraints**:

$$\max \text \quad \text \quad \text \leq \text$$

Where **Capability = Power × Agency**:
- **Power**: Ability to achieve diverse goals (what can the system accomplish?)
- **Agency**: Coherence of goal-pursuit (how optimizer-like is the system?)

See [Agent, Power, and Authority](/power-dynamics/agent-power-formalization/) for full formalization.

---

## Key Takeaways


> **Key Takeaways**
>
> 1. **The goal is maximizing capability subject to risk constraints**: Not minimizing risk to zero
> 2. **Capability = Power × Agency**: We want high power, minimum necessary agency
> 3. **Delegation Risk quantifies the downside**: Risk = Σ P(harm) × Damage
> 4. **Decompose, don't centralize**: Many limited components are safer than one powerful delegate
> 5. **Apply "Least X" principles**: Minimize capability, privilege, context, persistence, autonomy, connectivity
> 6. **Budget both risk AND power**: Allocate, verify, and enforce limits on both sides
> 7. **High power + low agency may be optimal**: "Strong tools" provide capability without coherent optimization pressure


## See Also

- [Introduction](/getting-started/introduction/) — The full problem statement and approach
- [Delegation Risk Overview](/delegation-risk/overview/) — The quantitative foundation
- [Least X Principles](/design-patterns/least-x-principles/) — Deep dive into each principle
- [Quick Start](/design-patterns/tools/quick-start/) — Apply these concepts step-by-step


------------------------------------------------------------
## Introduction
Source: getting-started/introduction.md
------------------------------------------------------------

# Introduction

## The Problem

Every delegation involves risk. When you delegate a task—to an employee, a contractor, a software system, or an AI agent—you're accepting potential downside in exchange for capability you don't have or can't apply yourself.

This is straightforward but worth stating precisely: **Delegation Risk is the expected cost of delegating**. For each way things could go wrong (we'll call these *harm modes*), multiply the probability by the damage. Sum across all harm modes:

```
Delegation Risk = Σ P(harm mode) × Damage(harm mode)
```

A research assistant that might leak proprietary data (P=0.001, damage=$50,000) contributes $50 to your Delegation Risk. An AI code deployer that might deploy malicious code (P=0.0001, damage=$1,000,000) contributes $100. You're implicitly accepting these costs when you delegate.

This framing has two virtues:

1. **It's quantifiable.** You can compare delegates, track changes over time, and make principled trade-offs. "Is this AI system safer?" becomes a question with a numerical answer—at least in principle, and sometimes in practice.

2. **It separates the problem from the solution.** You might reduce Delegation Risk by selecting more trustworthy delegates, by adding verification layers, or by limiting what delegates can do. The metric doesn't assume a particular approach.

Of course, the hard part is estimating the probabilities and damages. We'll address this, but it's worth acknowledging upfront: Delegation Risk is only as good as your estimates. The framework provides structure for reasoning about risk; it doesn't eliminate uncertainty.

## Why AI Systems?

AI systems may present delegation challenges at unusual scale. Several factors suggest this:

**Capabilities are expanding rapidly.** Whether verification is keeping pace is unclear—but there's at least reason for concern. A year ago, AI couldn't reliably write working code; now it often can. Our tools for verifying AI behavior have improved, but it's not obvious they've improved proportionally.

**Agent-to-agent delegation creates complex risk networks.** When AI agents delegate to other AI agents, risk relationships become networks rather than chains. If your AI assistant uses a plugin that calls another API that invokes another model—how much Delegation Risk are you accepting? Most current systems don't have principled answers to this question.

**Autonomy may increase while oversight decreases.** There are economic pressures toward more autonomous AI ([Gwern, 2016](https://gwern.net/tool-ai)), though how strong these pressures are and whether they'll dominate is uncertain. To the extent autonomy increases without proportional accountability, structural guarantees become more important.

**We may be building systems we don't fully understand.** To the extent AI systems have capabilities we haven't characterized, they have harm modes we haven't enumerated. The Delegation Risk calculation requires listing failure modes—uncertainty about capabilities translates directly to uncertainty about risk.

None of these claims are certain. But if even some of them hold, having infrastructure for managing Delegation Risk seems valuable. And even if AI turns out to be easier to manage than feared, the framework applies to delegation generally—the AI application is primary but not exclusive.

## The Approach

This framework proposes **structural constraints** as a foundation for managing Delegation Risk. The intuition: rather than relying solely on selecting trustworthy delegates or overseeing every action, we can design systems where dangerous behavior is difficult or impossible regardless of intentions.

The core idea: **safety can be a property of system architecture, not just individual component behavior**.

If you can build systems where:
- No single delegate has enough power to cause catastrophic harm
- Delegates can't easily coordinate to amplify their power
- Risk relationships are explicit and bounded
- Failures are contained and recoverable

...then you may not need perfect trust in any individual component.

This isn't a new idea. It's how nuclear plants aim for very low failure probabilities—by flowing system-level targets down to component budgets through fault trees. It's how financial institutions attempt to manage systemic risk. It's how secure systems try to limit blast radius through least privilege. It's also the logic behind constitutional separation of powers: bounding what any individual actor can do, regardless of their intentions.

Whether these approaches actually work as well as claimed is a separate question—nuclear safety has an impressive track record; financial risk management has a more mixed one. But the underlying principle of structural safety has proven useful across domains, and seems worth applying to AI systems.

What's newer here is attempting to systematize these ideas for AI specifically, where the delegates are less predictable than traditional software and the stakes may be higher than traditional human delegation.

## What This Framework Provides

**1. A vocabulary** for discussing delegation, risk, and containment. Precise terms help precise thinking.

**2. Mathematics** for quantifying risk, modeling how it propagates through delegation chains, and reasoning about allocations. The math is straightforward—mostly probability and expected value—but having explicit formulas helps avoid hand-waving.

**3. Principles** for constraining components. The "Least X" family (least privilege, least capability, least context, least persistence, least autonomy, least connectivity) provides a checklist for limiting what delegates can do, know, remember, and coordinate.

**4. Cross-domain methods** adapted from nuclear safety (fault trees, probabilistic risk assessment), finance (Euler allocation, coherent risk measures), and mechanism design (incentive-compatible reporting). These fields have decades of experience managing quantified risk; we can learn from their successes and failures.

**5. Patterns** for decomposing AI systems into components with bounded Delegation Risk. Concrete architectures, not just abstract principles.

Whether this all adds up to something useful is ultimately an empirical question. The framework provides structure; whether that structure helps in practice depends on details we can't fully anticipate.

## Scope and Limitations

This framework is primarily about **structural safety**—architectural properties that bound harm regardless of behavior. It complements, rather than replaces:

- **Alignment research**: Making AI systems that actually want what we want
- **Interpretability**: Understanding what AI systems are doing internally
- **AI Control**: Protocols for maintaining safety despite potential scheming ([Greenblatt et al., 2024](https://arxiv.org/abs/2312.06942))

The implicit bet is that structural constraints can provide meaningful safety guarantees even if alignment is imperfect and interpretability is limited—at least for some period where AI systems are capable enough to be concerning but not so capable that no containment is possible.

This bet might be wrong. Structural constraints might be less effective than hoped, or the window where they're useful might be shorter than expected, or the overhead of implementing them might outweigh the benefits. We think the approach is promising enough to develop, but we're not certain it will work.

The framework is also not a complete solution to any problem. It's infrastructure—a set of concepts, tools, and patterns that might help. How much it helps depends on how well it's applied and whether the underlying assumptions hold.

### What This Framework Does NOT Provide

To set expectations clearly:

- **A solution to AI alignment**: This is about containment, not making AI want the right things
- **Guaranteed safety**: Structural constraints can fail; this provides defense in depth, not certainty
- **Empirically validated protocols**: The AI-specific application is novel and largely untested at scale
- **Complete coverage of all AI risks**: Many risks (e.g., emergent capabilities, deceptive alignment) may not be addressed by structural approaches
- **A substitute for careful judgment**: The framework provides structure for thinking, not answers

## Domain Generality

While AI is our primary application, the framework is domain-general. The same mathematics applies to:

- **Organizational trust**: How much Delegation Risk does a CFO represent? How should authority be distributed across an organization?
- **Software systems**: How much can a microservice damage if compromised? How should permissions be allocated?
- **Supply chains**: How much exposure do you have to a supplier's failures? How should you diversify?
- **Government**: How much authority has been delegated to an agency? What structural checks exist?

We include examples from these domains for two reasons. First, they're often more intuitive than AI examples—most people have better intuitions about organizational trust than AI agent architectures. Second, decades of experience managing human delegation provides tested patterns. Some of these patterns may transfer to AI; others may not. But it seems worth learning from existing practice rather than starting from scratch.

## How to Read This

The documentation is organized from foundations to applications:

**Getting Started** provides overviews at various levels of detail—from a 5-minute summary to a full conceptual introduction.

**Delegation Risk** develops the mathematics: how to quantify Delegation Risk, how risk propagates through delegation chains, how to decompose and allocate risk budgets.

**Design Patterns** provides actionable guidance: the Least X principles, decomposed architectures, safety mechanisms, worked examples.

**Cross-Domain Methods** covers techniques borrowed from other fields: nuclear safety's probabilistic risk assessment, finance's Euler allocation, mechanism design's incentive-compatible reporting.

**Case Studies** applies the framework to specific scenarios: AI systems that succeeded or failed, human organizations, historical examples.

**Research** goes further into theory and background research for readers who want more depth.

You don't need to read linearly. If you want the core ideas fast, start with the [Five-Minute Intro](/getting-started/five-minute-intro/). If you want to apply the framework immediately, try the [Quick Start](/design-patterns/tools/quick-start/). If you want the full picture, the [Core Concepts](/getting-started/core-concepts/) page provides a visual overview of how everything connects.

---

## Key Takeaways


> **Key Takeaways**
>
> 1. **Risk is quantifiable**: Delegation Risk = Σ P(harm) × Damage
> 2. **Safety can be structural**: Architectural constraints can bound harm regardless of delegate behavior
> 3. **Decomposition limits risk**: No single component should have enough power to cause catastrophe
> 4. **Cross-domain methods may help**: Nuclear, finance, and mechanism design have relevant experience
> 5. **AI is our primary application**: The framework is general but AI systems are our focus
> 6. **Uncertainty remains**: The framework provides structure, not certainty


## Further Reading

### Key Background
- Gwern. [*Why Tool AIs Want to Be Agent AIs*](https://gwern.net/tool-ai) — Economic pressures toward agentic AI
- Greenblatt, R., et al. (2024). [*AI Control*](https://arxiv.org/abs/2312.06942) — Safety despite potential scheming
- Drexler, K.E. (2019). [*Comprehensive AI Services*](https://www.fhi.ox.ac.uk/reframing/) — Narrow services approach

### Related Concepts
- [Defense in Depth](https://en.wikipedia.org/wiki/Defense_in_depth_(computing)) — Multiple independent safety barriers
- [Separation of Powers](https://en.wikipedia.org/wiki/Separation_of_powers) — Distributing authority to prevent concentration
- [Principal-Agent Problem](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem) — Information asymmetry in delegation

See the [full bibliography](/reference/bibliography/) for comprehensive references.

---

## See Also

- [Core Concepts](/getting-started/core-concepts/) — Key ideas without mathematical formalism
- [Quick Start](/design-patterns/tools/quick-start/) — Step-by-step application checklist
- [Glossary](/getting-started/glossary/) — Terminology reference


------------------------------------------------------------
## FAQ & Common Objections
Source: getting-started/faq.md
------------------------------------------------------------

# FAQ & Common Objections

Answers to frequently asked questions and responses to common skepticism.

## Conceptual Questions

### "Isn't this just defense in depth? That's not new."

Yes, defense in depth is the core principle—but applied systematically to AI systems with quantified delegation risk budgets. The novelty is:

1. **Trust as the primary resource**: Not just "add more checks" but "budget expected harm across components"
2. **Formal propagation rules**: Mathematical framework for how trust compounds through delegation chains
3. **AI-specific constraints**: The "Least X" principles address AI-unique risks like context accumulation and optimization pressure

Defense in depth tells you *what* to do. This framework attempts to provide structure for reasoning about *how much* and *where*—though the estimates are often uncertain.

### "How is Delegation Risk different from regular risk assessment?"

Delegation Risk (Delegation Risk) is a specific instantiation of expected value calculation:

```
Delegation Risk = Σ P(outcome) × Damage(outcome)
```

The key differences from traditional risk assessment:

| Traditional Risk | Delegation Risk Approach |
|-----------------|--------------|
| Often qualitative (High/Medium/Low) | Always quantified (dollars) |
| Per-hazard analysis | Per-component allocation |
| Static assessment | Dynamic budget tracking |
| Compliance-focused | Optimization-focused |

Delegation Risk lets you make tradeoffs: "Is adding verification worth $50/month in reduced Delegation Risk?"

### "Why decomposition instead of one aligned agent?"

Two reasons:

1. **Alignment is uncertain**: We don't know how to guarantee alignment. Decomposition limits damage if we're wrong.

2. **Verification is easier**: Verifying a narrow component is tractable. Verifying a general agent's behavior across all possible inputs is not.

Even if we solved alignment tomorrow, decomposition would still be valuable for auditability, modularity, and incremental deployment.

### "Can't a sufficiently intelligent system route around these constraints?"

Theoretically, yes—if it has:
- Enough context to understand the constraints
- Enough capability to find exploits
- Enough persistence to execute multi-step plans
- Enough connectivity to coordinate attack

The "Least X" principles specifically bound each of these. A component that doesn't know it's constrained, can't remember previous invocations, and can't communicate with other components has limited ability to "route around."

This isn't a proof of safety—it's defense in depth that makes attacks harder.

### "Doesn't this severely limit what AI can do?"

Yes, deliberately. The question is: **limited compared to what?**

| Approach | Capability | Safety |
|----------|------------|--------|
| Unrestricted agent | Maximum | Minimal |
| Decomposed system | High (but slower) | Bounded |
| Tool AI only | Limited | Higher |

Decomposed systems may be able to accomplish many tasks—they do it through many narrow components rather than one powerful agent. The tradeoff is latency and complexity; how much capability is lost depends on the specific task and implementation.

---

## Practical Questions

### "How do I estimate failure probabilities?"

Start with rough estimates and refine:

| Estimate Type | When to Use |
|--------------|-------------|
| **Historical data** | If you have logs of similar systems |
| **Red team results** | Run adversarial testing, measure failure rate |
| **Expert judgment** | For novel systems, calibrated estimates |
| **Conservative defaults** | P=0.1 for LLM outputs, P=0.01 for code |

The specific number matters less than:
1. Consistent methodology across components
2. Regular updates as you gather data
3. Conservative bias (overestimate risk)

### "What's the minimum I need to do?"

For low-risk internal tools:

1. **List components** (5 min)
2. **Estimate worst-case damage** per component (10 min)
3. **Apply basic Least X checklist** (15 min)
4. **Add monitoring** (varies)

See the [Quick Start](/design-patterns/tools/quick-start/) for the full process, but you can start with just steps 1-3.

### "How does this work with existing ML infrastructure?"

This framework is infrastructure-agnostic. Components can be:

- API calls to hosted models (OpenAI, Anthropic)
- Self-hosted models (Llama, Mistral)
- Traditional code (Python, TypeScript)
- Hybrid systems

The constraints are applied at the **interface** level—permissions, context windows, output validation—not the implementation.

### "What tools support this?"

Currently, implementing requires manual engineering. Useful building blocks:

- **LangChain/LlamaIndex**: Component orchestration
- **Guardrails AI**: Output validation
- **OpenTelemetry**: Monitoring and tracing
- **Semgrep**: Static analysis for code components

See the [Interactive Calculators](/design-patterns/tools/delegation-risk-calculator/) for Delegation Risk computation.

### "How do I convince my team/manager to adopt this?"

Focus on risk reduction ROI:

1. **Quantify current risk**: "Our current system has ~$X/month expected harm"
2. **Show mitigation cost**: "Adding verification reduces this to $Y for Z engineering effort"
3. **Compare alternatives**: "Versus incident response, prevention is N times cheaper"

The Delegation Risk framework gives you concrete numbers to make the business case.

---

## Skeptical Questions

### "This seems like a lot of overhead for uncertain benefit."

Fair concern. The framework is most valuable when:

| Situation | Framework Value |
|-----------|-----------------|
| Low-stakes internal tools | Low—simple safeguards sufficient |
| Customer-facing products | Medium—reputation/liability risk |
| Autonomous systems | High—limited human oversight |
| Safety-critical applications | Essential—regulatory/catastrophic risk |

Start with high-risk components. Apply lighter versions elsewhere.

### "Don't the Least X principles conflict with each other?"

Sometimes, yes. For example:
- **Least Intelligence** vs. **task requirements**: Some tasks need capable models
- **Least Context** vs. **task performance**: More context often improves quality

The resolution: **apply constraints proportionate to risk**. A public FAQ bot needs less constraint than a code deployment system.

Use the [Decision Guide](/design-patterns/tools/decision-guide/) to navigate tradeoffs.

### "Why trust old/dumb models for coordination when they make more mistakes?"

Two types of mistakes:

1. **Capability errors**: Wrong answer due to insufficient intelligence
2. **Alignment failures**: Correct execution of wrong goal (including deception)

Old/dumb models make more Type 1 errors but fewer Type 2 errors. For coordination:
- Type 1 errors → caught by verification layers
- Type 2 errors → potentially catastrophic, hard to detect

The framework accepts higher capability error rates in exchange for lower alignment failure rates.

### "How do you know this actually works?"

Honestly: **we don't have proof for AI systems specifically**. The evidence is:

1. **Analogous domains**: Nuclear, finance, and aerospace use similar principles with demonstrated success
2. **Logical argument**: Limiting capability, context, and connectivity necessarily limits attack surface
3. **Red team results**: Systems designed with these constraints are harder to exploit

The [Empirical Tests](/design-patterns/tools/empirical-tests/) page outlines how to gather evidence for your specific deployment.

### "Isn't this just security theater?"

Security theater provides the *appearance* of security without substance. This framework differs:

| Security Theater | This Framework |
|-----------------|----------------|
| Qualitative ("more secure") | Quantified (Delegation Risk in dollars) |
| Checkbox compliance | Budget allocation and monitoring |
| Static assessment | Continuous measurement |
| Unfalsifiable claims | Testable predictions |

If your Delegation Risk estimates are wrong, production data should eventually show it. That's the intent, at least—whether it works in practice for AI systems specifically is still being validated.

---

## Technical Questions

### "How does risk inheritance handle cycles?"

For acyclic graphs: straightforward path calculation.

For cycles: several approaches:
1. **Iteration to convergence**: Repeatedly apply propagation until stable
2. **Maximum path length**: Ignore paths longer than k
3. **Damping factor**: Each hop reduces trust (like PageRank)

Most practical systems should be acyclic by design—cycles indicate potential coordination channels.

### "What about emergent behavior from component interactions?"

Emergent behavior is limited by:
1. **Information bottlenecks**: Components can only share structured outputs
2. **No direct communication**: Must go through verified channels
3. **Independent implementations**: Different components, different failure modes

This doesn't prevent all emergence, but bounds the complexity of emergent behavior to what can be expressed through the defined interfaces.

### "How do you handle adversarial users, not just adversarial AI?"

Same principles apply:
- **Least Privilege**: Users get minimum necessary access
- **Input validation**: All inputs sanitized/validated
- **Rate limiting**: Prevent rapid exploitation attempts
- **Monitoring**: Detect anomalous usage patterns

The framework handles both AI-origin and human-origin threats.

---

## See Also

- [Quick Start](/design-patterns/tools/quick-start/) — Practical application checklist
- [Decision Guide](/design-patterns/tools/decision-guide/) — Navigating tradeoffs
- [Related Approaches](/reference/related-approaches/) — How this compares to alternatives
- [Empirical Tests](/design-patterns/tools/empirical-tests/) — Validation methodology
