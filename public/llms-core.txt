# Delegation Risk Framework - Core Documentation

> Version: 1.0.2 | Generated: 2025-12-23
>
> Essential pages for understanding the framework.
> For complete documentation, see: https://delegation-risk.org

================================================================================


------------------------------------------------------------
## Getting Started
Source: getting-started/index.md
------------------------------------------------------------

This section helps you understand Delegation Risk at whatever depth you need.

## Choose Your Path

### Quick Understanding (5-10 minutes)
If you want the core ideas fast:
1. **[Five-Minute Intro](/getting-started/five-minute-intro/)** — The essential concepts in one page

### Solid Foundation (30-60 minutes)
If you're evaluating whether this framework is useful:
1. **[Introduction](/getting-started/introduction/)** — The problem we're solving and our approach
2. **[Core Concepts](/getting-started/core-concepts/)** — Key ideas with visual framework diagram
3. **[FAQ](/getting-started/faq/)** — Common objections answered

### Ready to Apply (2-3 hours)
If you want to use this framework on a real system:
1. **[Introduction](/getting-started/introduction/)** — Problem context
2. **[Core Concepts](/getting-started/core-concepts/)** — The conceptual foundation
3. **[Delegation Risk](/delegation-risk/)** — Quantitative details
4. **[Quick Start](/design-patterns/tools/quick-start/)** — Step-by-step application checklist

## Section Contents

| Page | Purpose | Reading Time |
|------|---------|--------------|
| [Introduction](/getting-started/introduction/) | Why delegation risk matters, the core problem | 10 min |
| [Five-Minute Intro](/getting-started/five-minute-intro/) | Compressed overview for busy readers | 5 min |
| [Core Concepts](/getting-started/core-concepts/) | Visual framework, key ideas without math | 20 min |
| [FAQ](/getting-started/faq/) | Objections, edge cases, clarifications | 15 min |
| [Glossary](/getting-started/glossary/) | Term definitions (reference) | — |

## How This Section Connects


[Diagram: flowchart TB]


## Next Steps

Once you understand the basics:
- **Want the math?** → [Delegation Risk](/delegation-risk/) for quantitative foundations
- **Want to apply it?** → [Design Patterns](/design-patterns/) for practical guidance
- **Want examples?** → [Case Studies](/case-studies/) for real-world applications


------------------------------------------------------------
## 5-Minute Introduction
Source: getting-started/five-minute-intro.md
------------------------------------------------------------

# Delegation Risk in 5 Minutes


> **NOTE**
>
> This is the compressed version. For caveats and nuance, see the [full Introduction](/getting-started/introduction/).


**The problem**: AI agents can cause real harm. One powerful agent with full access is risky—if it fails or schemes, damage may be unbounded.

**The proposed solution**: Decompose into many limited components. Budget risk like money.

---

## The Core Idea

Instead of this (stylized—real systems exist on a spectrum):

```
User → [Powerful AI Agent] → Any Action
              ↑
        Full context
        Full capability
        Full autonomy
```

Consider this:

```
User → [Router] → [Narrow Component 1] → [Verifier] → Limited Action 1
                → [Narrow Component 2] → [Verifier] → Limited Action 2
                → [Narrow Component 3] → [Human Gate] → Sensitive Action
```

Each component has:
- **Limited capability** (can only do specific things)
- **Limited context** (doesn't see the whole system)
- **Limited autonomy** (verified or human-approved)

---

## The Formula

**Delegation Risk** = probability of bad outcome × damage

```
Delegation Risk = Σ P(failure) × Damage
```

Example: Component has 1% chance of causing $10,000 damage → Delegation Risk = $100

**Budget your Delegation Risk** across components. If total exceeds budget, add safeguards.

---

## The Principles

Apply "Least X" to every component:

| Principle | Meaning |
|-----------|---------|
| **Least Intelligence** | Dumbest implementation that works |
| **Least Privilege** | Minimum permissions |
| **Least Context** | Minimum information |
| **Least Persistence** | No memory between calls |
| **Least Autonomy** | Human oversight where needed |

---

## The Hierarchy

Use the most verifiable option:

```
Best → Verified Code → Regular Code → Narrow Models → General LLMs → Worst
```

Don't use GPT-4 for things a regex can do.

---

## Quick Example

**Task**: Slack bot answering questions from docs

| Component | Implementation | Delegation Risk |
|-----------|---------------|-----|
| Retriever | Code (vector search) | $5/mo |
| Answerer | Fine-tuned 7B | $50/mo |
| Poster | Code (rate limited) | $1/mo |
| **Total** | | **$56/mo** |

Budget: $500/mo → **Within budget. Ship it.**

---

## When to Use This

| Situation | Framework Value |
|-----------|-----------------|
| Internal tools, recoverable mistakes | Light application |
| Customer-facing products | Medium rigor |
| Autonomous agents with real-world actions | Full application |
| Safety-critical systems | Essential |

---

## Next Steps

- **Want the full picture?** → [Core Concepts](/getting-started/core-concepts/)
- **Ready to apply it?** → [Quick Start](/design-patterns/tools/quick-start/)
- **Want the math?** → [Delegation Risk](/delegation-risk/overview/)
- **Have questions?** → [FAQ](/getting-started/faq/)

---

**TL;DR**: Consider building many limited components instead of one powerful AI. Quantify risk. Budget it like money.


------------------------------------------------------------
## Core Concepts
Source: getting-started/core-concepts.md
------------------------------------------------------------

# Core Concepts

This page introduces the key ideas without mathematical formalism. For quantitative details, see [Delegation Risk](/delegation-risk/overview/).

## Framework Overview

How all the pieces fit together:


[Diagram: flowchart TB]


**Reading the diagram**: Delegation Risk is the foundation. Theory explains how it behaves. Methods provide established approaches from other fields. Principles give actionable constraints. Application shows how to build systems (with AI as primary focus).

## Delegation Risk

When you delegate a task, you're accepting **risk**—the potential for harm if the delegate fails or misbehaves.

**Delegation Risk** quantifies this: for each delegate, sum over all possible bad outcomes, weighted by their probability. A delegate with access to your bank account has higher Delegation Risk than one that can only read public web pages.


> **TIP**
>
> Risk is finite and should be budgeted. Just as organizations allocate compute and money, they should allocate delegation risk—tracking how much they're accepting, from which delegates, for what purposes.


### The Formula

**Delegation Risk = Σ P(harm mode) × Damage(harm mode)**

For each way something could go wrong (harm mode), multiply:
- The probability it happens
- The damage if it does

Sum across all harm modes to get total Delegation Risk.

### Example: Employee with Check-Writing Authority

| Harm Mode | Probability | Damage | Risk Contribution |
|-----------|-------------|--------|-------------------|
| Clerical error | 0.01 | $1,000 | $10 |
| Unauthorized payment | 0.001 | $50,000 | $50 |
| Fraud | 0.0001 | $500,000 | $50 |

**Delegation Risk** = $10 + $50 + $50 = **$110** expected cost

This doesn't mean you'll lose $110. It means that over many such delegations, you'd expect $110 in losses on average. You can compare this to the value the delegation provides and decide if it's worth it.

## Risk Propagation

When delegates delegate to other delegates, risk relationships form networks. If A trusts B with weight 0.8, and B trusts C with weight 0.7, how much should A trust C?

Different rules give different answers:
- **Multiplicative**: 0.8 × 0.7 = 0.56 (each stage is independent risk)
- **Minimum**: min(0.8, 0.7) = 0.7 (chain only as strong as weakest link)
- **Discounted**: More complex models accounting for path length


[Diagram: graph LR]


The right rule depends on what "trust" means in your context. The framework provides formal tools for reasoning about these relationships.


**Choosing a propagation rule**


| Rule | Formula | Best when... |
|------|---------|--------------|
| **Multiplicative** | T(A→C) = T(A→B) × T(B→C) | Each delegation is independent risk; default choice |
| **Minimum** | T(A→C) = min(T(A→B), T(B→C)) | Chain only as strong as weakest link |
| **Discounted** | T(A→C) = T(A→B) × T(B→C) × δⁿ | Longer paths inherently less trustworthy |
| **Maximum path** | T(A→C) = max over all paths | Any valid route suffices |

Most systems should start with **multiplicative** propagation—it's intuitive, conservative, and composes well.


> **Related: Social Choice Theory**
>
> Trust aggregation across multiple delegates relates to [social choice theory](https://en.wikipedia.org/wiki/Social_choice_theory)—how to combine individual inputs into collective outcomes. [Arrow's impossibility theorem](https://en.wikipedia.org/wiki/Arrow%27s_impossibility_theorem) has implications for trust aggregation: no perfect rule exists.


## Decomposition Over Monoliths

A single powerful delegate is dangerous because it concentrates capability, context, and control. If it fails or acts against you, the damage is unbounded.

**Decomposition** breaks this pattern:
- Split tasks across many specialized components
- Each component has limited capability and context
- No single component can cause catastrophic harm
- Components can't easily coordinate against the system

Think of it like [separation of powers](https://en.wikipedia.org/wiki/Separation_of_powers) in government, or [microservices](https://en.wikipedia.org/wiki/Microservices) in software architecture.

### Example: Organizational Analogy

| Aspect | Centralized | Decomposed |
|--------|-------------|------------|
| **Single point of failure** | CEO makes all decisions | Authority distributed across roles |
| **Blast radius** | Bad CEO decision affects everything | Bad decision contained to department |
| **Coordination risk** | CEO can pursue personal agenda | Requires multiple people to collude |
| **Verification** | Must verify CEO's judgment on everything | Verify narrow, specific behaviors |

The same logic applies to software systems and AI agents.

## The "Least X" Principles

Security engineering has the [principle of least privilege](https://en.wikipedia.org/wiki/Principle_of_least_privilege): give each component minimum necessary access. We extend this across multiple dimensions:

- **Least Capability**: Minimum ability to take actions
- **Least Privilege**: Minimum permissions and access rights
- **Least Context**: Minimum information about the broader system
- **Least Persistence**: Minimum state maintained across invocations
- **Least Autonomy**: Minimum ability to act without oversight
- **Least Connectivity**: Minimum communication paths between components

Each principle bounds a different attack surface. Applied together, they create components that are limited in what they can do, know, remember, and coordinate.


**Why each principle matters**


| Principle | What it limits | Risk bounded |
|-----------|---------------|--------------|
| Least Capability | Ability to act | Direct damage from actions |
| Least Privilege | Access rights | Scope of potential harm |
| Least Context | System knowledge | Strategic planning against system |
| Least Persistence | Memory across calls | Long-term planning, accumulating leverage |
| Least Autonomy | Unsupervised action | Scope of damage before intervention |
| Least Connectivity | Communication paths | Coordination between components |


## Risk Budgeting

Borrowed from finance and nuclear safety: set a total acceptable risk level for the system, then allocate risk budgets to components.

Key requirements:
- **Compositional guarantees**: Component risks must aggregate predictably
- **Principled allocation**: Methods like [Euler allocation](/cross-domain-methods/euler-allocation/) ensure budgets sum correctly
- **Incentive-compatible reporting**: Mechanisms that make honest risk reporting optimal
- **Verification infrastructure**: Independent confirmation that claimed levels match reality
- **Conservative margins**: Buffer for uncertainty and unknowns


> **NOTE**
>
> This isn't theoretical—nuclear plants operate at 10⁻⁹ failure probability per reactor-year by flowing system targets down to component budgets through fault trees.


[Diagram: flowchart TD]


## Architectural Safety

The central claim: safety can be a property of system architecture, not just individual component behavior.

If you can't guarantee a delegate is trustworthy, you can still:
- Limit what they can access (least privilege)
- Limit what they know (least context)
- Limit what they can coordinate (decomposition)
- Limit how much damage they can cause (risk budgets)
- Verify their behavior matches claims (verification layers)


> **CAUTION**
>
> These structural constraints provide defense in depth. They don't replace selecting trustworthy delegates—they bound the damage when trust is misplaced.


---

## Application to AI Systems

The framework applies generally, but AI systems are our primary focus. For AI specifically:

### Verifiability Hierarchy

Not all implementations are equally trustworthy:

1. **Provably secure code** — Mathematical guarantees of correctness
2. **Regular code** — Auditable, testable, deterministic
3. **Fine-tuned narrow models** — Predictable within a specific domain
4. **Constrained general models** — LLMs with extensive prompting and evaluation
5. **Base LLMs** — General capability, harder to verify
6. **RL systems** — Learned policies, least predictable


> **Design Principle**
>
> Use the most verifiable implementation that achieves the required functionality. Don't use a frontier LLM for tasks that can be done with code or a narrow model.


[Diagram: graph TB]


**When to use each level**


| Level | Use when... | Example |
|-------|-------------|---------|
| Provably secure code | Correctness is critical, logic is simple | Access control checks, rate limiters |
| Regular code | Behavior is fully specifiable | Data transformation, API routing |
| Fine-tuned narrow models | Task is well-defined, training data exists | Sentiment classification, entity extraction |
| Constrained general models | Judgment needed, outcomes recoverable | Draft generation, summarization |
| Base LLMs | Maximum flexibility needed, heavy oversight | Creative tasks, complex reasoning |
| RL systems | Learning from feedback is essential | Rarely recommended for safety-critical paths |


### Architectural Spectrum: Monolithic vs Decomposed

Real systems exist on a spectrum. This diagram shows two stylized endpoints:


[Diagram: flowchart TB]


| Aspect | Monolithic | Decomposed |
|--------|------------|------------|
| **Single point of failure** | Yes—one bad output, system fails | No—failure isolated to component |
| **Attack surface** | Entire model capability | Only exposed component interfaces |
| **Verification** | Must verify everything the model can do | Verify narrow, specific behaviors |
| **Blast radius** | Unlimited—model can do anything | Bounded—each component has limits |
| **Coordination risk** | Model coordinates with itself | Components can't easily collude |

---

## What This Framework Provides

1. **Vocabulary**: Precise terms for discussing risk, delegation, and containment
2. **Theory**: Mathematics for risk propagation, capability quantification, and optimization
3. **Methods**: Established approaches adapted from finance, nuclear safety, mechanism design
4. **Principles**: Actionable design constraints (the "Least X" family)
5. **Patterns**: Decomposed coordination, verification layers, safety mechanisms

The goal is infrastructure for safely delegating at scale—not a complete solution to trust, but a foundation for managing risk as capabilities increase.

### The Optimization Problem

The framework isn't just about minimizing risk—it's about **maximizing capability subject to risk constraints**:

$$\max \text \quad \text \quad \text \leq \text$$

Where **Capability = Power × Agency**:
- **Power**: Ability to achieve diverse goals (what can the system accomplish?)
- **Agency**: Coherence of goal-pursuit (how optimizer-like is the system?)

See [Agent, Power, and Authority](/power-dynamics/agent-power-formalization/) for full formalization.

---

## Key Takeaways


> **Key Takeaways**
>
> 1. **The goal is maximizing capability subject to risk constraints**: Not minimizing risk to zero
> 2. **Capability = Power × Agency**: We want high power, minimum necessary agency
> 3. **Delegation Risk quantifies the downside**: Risk = Σ P(harm) × Damage
> 4. **Decompose, don't centralize**: Many limited components are safer than one powerful delegate
> 5. **Apply "Least X" principles**: Minimize capability, privilege, context, persistence, autonomy, connectivity
> 6. **Budget both risk AND power**: Allocate, verify, and enforce limits on both sides
> 7. **High power + low agency may be optimal**: "Strong tools" provide capability without coherent optimization pressure


## See Also

- [Introduction](/getting-started/introduction/) — The full problem statement and approach
- [Delegation Risk Overview](/delegation-risk/overview/) — The quantitative foundation
- [Least X Principles](/design-patterns/least-x-principles/) — Deep dive into each principle
- [Quick Start](/design-patterns/tools/quick-start/) — Apply these concepts step-by-step


------------------------------------------------------------
## Introduction
Source: getting-started/introduction.md
------------------------------------------------------------

# Introduction

## The Problem

Every delegation involves risk. When you delegate a task—to an employee, a contractor, a software system, or an AI agent—you're accepting potential downside in exchange for capability you don't have or can't apply yourself.

This is straightforward but worth stating precisely: **Delegation Risk is the expected cost of delegating**. For each way things could go wrong (we'll call these *harm modes*), multiply the probability by the damage. Sum across all harm modes:

```
Delegation Risk = Σ P(harm mode) × Damage(harm mode)
```

A research assistant that might leak proprietary data (P=0.001, damage=$50,000) contributes $50 to your Delegation Risk. An AI code deployer that might deploy malicious code (P=0.0001, damage=$1,000,000) contributes $100. You're implicitly accepting these costs when you delegate.

This framing has two virtues:

1. **It's quantifiable.** You can compare delegates, track changes over time, and make principled trade-offs. "Is this AI system safer?" becomes a question with a numerical answer—at least in principle, and sometimes in practice.

2. **It separates the problem from the solution.** You might reduce Delegation Risk by selecting more trustworthy delegates, by adding verification layers, or by limiting what delegates can do. The metric doesn't assume a particular approach.

Of course, the hard part is estimating the probabilities and damages. We'll address this, but it's worth acknowledging upfront: Delegation Risk is only as good as your estimates. The framework provides structure for reasoning about risk; it doesn't eliminate uncertainty.

## Why AI Systems?

AI systems may present delegation challenges at unusual scale. Several factors suggest this:

**Capabilities are expanding rapidly.** Whether verification is keeping pace is unclear—but there's at least reason for concern. A year ago, AI couldn't reliably write working code; now it often can. Our tools for verifying AI behavior have improved, but it's not obvious they've improved proportionally.

**Agent-to-agent delegation creates complex risk networks.** When AI agents delegate to other AI agents, risk relationships become networks rather than chains. If your AI assistant uses a plugin that calls another API that invokes another model—how much Delegation Risk are you accepting? Most current systems don't have principled answers to this question.

**Autonomy may increase while oversight decreases.** There are economic pressures toward more autonomous AI ([Gwern, 2016](https://gwern.net/tool-ai)), though how strong these pressures are and whether they'll dominate is uncertain. To the extent autonomy increases without proportional accountability, structural guarantees become more important.

**We may be building systems we don't fully understand.** To the extent AI systems have capabilities we haven't characterized, they have harm modes we haven't enumerated. The Delegation Risk calculation requires listing failure modes—uncertainty about capabilities translates directly to uncertainty about risk.

None of these claims are certain. But if even some of them hold, having infrastructure for managing Delegation Risk seems valuable. And even if AI turns out to be easier to manage than feared, the framework applies to delegation generally—the AI application is primary but not exclusive.

## The Approach

This framework proposes **structural constraints** as a foundation for managing Delegation Risk. The intuition: rather than relying solely on selecting trustworthy delegates or overseeing every action, we can design systems where dangerous behavior is difficult or impossible regardless of intentions.

The core idea: **safety can be a property of system architecture, not just individual component behavior**.

If you can build systems where:
- No single delegate has enough power to cause catastrophic harm
- Delegates can't easily coordinate to amplify their power
- Risk relationships are explicit and bounded
- Failures are contained and recoverable

...then you may not need perfect trust in any individual component.

This isn't a new idea. It's how nuclear plants aim for very low failure probabilities—by flowing system-level targets down to component budgets through fault trees. It's how financial institutions attempt to manage systemic risk. It's how secure systems try to limit blast radius through least privilege. It's also the logic behind constitutional separation of powers: bounding what any individual actor can do, regardless of their intentions.

Whether these approaches actually work as well as claimed is a separate question—nuclear safety has an impressive track record; financial risk management has a more mixed one. But the underlying principle of structural safety has proven useful across domains, and seems worth applying to AI systems.

What's newer here is attempting to systematize these ideas for AI specifically, where the delegates are less predictable than traditional software and the stakes may be higher than traditional human delegation.

## What This Framework Provides

**1. A vocabulary** for discussing delegation, risk, and containment. Precise terms help precise thinking.

**2. Mathematics** for quantifying risk, modeling how it propagates through delegation chains, and reasoning about allocations. The math is straightforward—mostly probability and expected value—but having explicit formulas helps avoid hand-waving.

**3. Principles** for constraining components. The "Least X" family (least privilege, least capability, least context, least persistence, least autonomy, least connectivity) provides a checklist for limiting what delegates can do, know, remember, and coordinate.

**4. Cross-domain methods** adapted from nuclear safety (fault trees, probabilistic risk assessment), finance (Euler allocation, coherent risk measures), and mechanism design (incentive-compatible reporting). These fields have decades of experience managing quantified risk; we can learn from their successes and failures.

**5. Patterns** for decomposing AI systems into components with bounded Delegation Risk. Concrete architectures, not just abstract principles.

Whether this all adds up to something useful is ultimately an empirical question. The framework provides structure; whether that structure helps in practice depends on details we can't fully anticipate.

## Scope and Limitations

This framework is primarily about **structural safety**—architectural properties that bound harm regardless of behavior. It complements, rather than replaces:

- **Alignment research**: Making AI systems that actually want what we want
- **Interpretability**: Understanding what AI systems are doing internally
- **AI Control**: Protocols for maintaining safety despite potential scheming ([Greenblatt et al., 2024](https://arxiv.org/abs/2312.06942))

The implicit bet is that structural constraints can provide meaningful safety guarantees even if alignment is imperfect and interpretability is limited—at least for some period where AI systems are capable enough to be concerning but not so capable that no containment is possible.

This bet might be wrong. Structural constraints might be less effective than hoped, or the window where they're useful might be shorter than expected, or the overhead of implementing them might outweigh the benefits. We think the approach is promising enough to develop, but we're not certain it will work.

The framework is also not a complete solution to any problem. It's infrastructure—a set of concepts, tools, and patterns that might help. How much it helps depends on how well it's applied and whether the underlying assumptions hold.

### What This Framework Does NOT Provide

To set expectations clearly:

- **A solution to AI alignment**: This is about containment, not making AI want the right things
- **Guaranteed safety**: Structural constraints can fail; this provides defense in depth, not certainty
- **Empirically validated protocols**: The AI-specific application is novel and largely untested at scale
- **Complete coverage of all AI risks**: Many risks (e.g., emergent capabilities, deceptive alignment) may not be addressed by structural approaches
- **A substitute for careful judgment**: The framework provides structure for thinking, not answers

## Domain Generality

While AI is our primary application, the framework is domain-general. The same mathematics applies to:

- **Organizational trust**: How much Delegation Risk does a CFO represent? How should authority be distributed across an organization?
- **Software systems**: How much can a microservice damage if compromised? How should permissions be allocated?
- **Supply chains**: How much exposure do you have to a supplier's failures? How should you diversify?
- **Government**: How much authority has been delegated to an agency? What structural checks exist?

We include examples from these domains for two reasons. First, they're often more intuitive than AI examples—most people have better intuitions about organizational trust than AI agent architectures. Second, decades of experience managing human delegation provides tested patterns. Some of these patterns may transfer to AI; others may not. But it seems worth learning from existing practice rather than starting from scratch.

## How to Read This

The documentation is organized from foundations to applications:

**Getting Started** provides overviews at various levels of detail—from a 5-minute summary to a full conceptual introduction.

**Delegation Risk** develops the mathematics: how to quantify Delegation Risk, how risk propagates through delegation chains, how to decompose and allocate risk budgets.

**Design Patterns** provides actionable guidance: the Least X principles, decomposed architectures, safety mechanisms, worked examples.

**Cross-Domain Methods** covers techniques borrowed from other fields: nuclear safety's probabilistic risk assessment, finance's Euler allocation, mechanism design's incentive-compatible reporting.

**Case Studies** applies the framework to specific scenarios: AI systems that succeeded or failed, human organizations, historical examples.

**Research** goes further into theory and background research for readers who want more depth.

You don't need to read linearly. If you want the core ideas fast, start with the [Five-Minute Intro](/getting-started/five-minute-intro/). If you want to apply the framework immediately, try the [Quick Start](/design-patterns/tools/quick-start/). If you want the full picture, the [Core Concepts](/getting-started/core-concepts/) page provides a visual overview of how everything connects.

---

## Key Takeaways


> **Key Takeaways**
>
> 1. **Risk is quantifiable**: Delegation Risk = Σ P(harm) × Damage
> 2. **Safety can be structural**: Architectural constraints can bound harm regardless of delegate behavior
> 3. **Decomposition limits risk**: No single component should have enough power to cause catastrophe
> 4. **Cross-domain methods may help**: Nuclear, finance, and mechanism design have relevant experience
> 5. **AI is our primary application**: The framework is general but AI systems are our focus
> 6. **Uncertainty remains**: The framework provides structure, not certainty


## Further Reading

### Key Background
- Gwern. [*Why Tool AIs Want to Be Agent AIs*](https://gwern.net/tool-ai) — Economic pressures toward agentic AI
- Greenblatt, R., et al. (2024). [*AI Control*](https://arxiv.org/abs/2312.06942) — Safety despite potential scheming
- Drexler, K.E. (2019). [*Comprehensive AI Services*](https://www.fhi.ox.ac.uk/reframing/) — Narrow services approach

### Related Concepts
- [Defense in Depth](https://en.wikipedia.org/wiki/Defense_in_depth_(computing)) — Multiple independent safety barriers
- [Separation of Powers](https://en.wikipedia.org/wiki/Separation_of_powers) — Distributing authority to prevent concentration
- [Principal-Agent Problem](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem) — Information asymmetry in delegation

See the [full bibliography](/reference/bibliography/) for comprehensive references.

---

## See Also

- [Core Concepts](/getting-started/core-concepts/) — Key ideas without mathematical formalism
- [Quick Start](/design-patterns/tools/quick-start/) — Step-by-step application checklist
- [Glossary](/getting-started/glossary/) — Terminology reference


------------------------------------------------------------
## FAQ & Common Objections
Source: getting-started/faq.md
------------------------------------------------------------

# FAQ & Common Objections

Answers to frequently asked questions and responses to common skepticism.

## Conceptual Questions

### "Isn't this just defense in depth? That's not new."

Yes, defense in depth is the core principle—but applied systematically to AI systems with quantified delegation risk budgets. The novelty is:

1. **Trust as the primary resource**: Not just "add more checks" but "budget expected harm across components"
2. **Formal propagation rules**: Mathematical framework for how trust compounds through delegation chains
3. **AI-specific constraints**: The "Least X" principles address AI-unique risks like context accumulation and optimization pressure

Defense in depth tells you *what* to do. This framework attempts to provide structure for reasoning about *how much* and *where*—though the estimates are often uncertain.

### "How is Delegation Risk different from regular risk assessment?"

Delegation Risk (Delegation Risk) is a specific instantiation of expected value calculation:

```
Delegation Risk = Σ P(outcome) × Damage(outcome)
```

The key differences from traditional risk assessment:

| Traditional Risk | Delegation Risk Approach |
|-----------------|--------------|
| Often qualitative (High/Medium/Low) | Always quantified (dollars) |
| Per-hazard analysis | Per-component allocation |
| Static assessment | Dynamic budget tracking |
| Compliance-focused | Optimization-focused |

Delegation Risk lets you make tradeoffs: "Is adding verification worth $50/month in reduced Delegation Risk?"

### "Why decomposition instead of one aligned agent?"

Two reasons:

1. **Alignment is uncertain**: We don't know how to guarantee alignment. Decomposition limits damage if we're wrong.

2. **Verification is easier**: Verifying a narrow component is tractable. Verifying a general agent's behavior across all possible inputs is not.

Even if we solved alignment tomorrow, decomposition would still be valuable for auditability, modularity, and incremental deployment.

### "Can't a sufficiently intelligent system route around these constraints?"

Theoretically, yes—if it has:
- Enough context to understand the constraints
- Enough capability to find exploits
- Enough persistence to execute multi-step plans
- Enough connectivity to coordinate attack

The "Least X" principles specifically bound each of these. A component that doesn't know it's constrained, can't remember previous invocations, and can't communicate with other components has limited ability to "route around."

This isn't a proof of safety—it's defense in depth that makes attacks harder.

### "Doesn't this severely limit what AI can do?"

Yes, deliberately. The question is: **limited compared to what?**

| Approach | Capability | Safety |
|----------|------------|--------|
| Unrestricted agent | Maximum | Minimal |
| Decomposed system | High (but slower) | Bounded |
| Tool AI only | Limited | Higher |

Decomposed systems may be able to accomplish many tasks—they do it through many narrow components rather than one powerful agent. The tradeoff is latency and complexity; how much capability is lost depends on the specific task and implementation.

---

## Practical Questions

### "How do I estimate failure probabilities?"

Start with rough estimates and refine:

| Estimate Type | When to Use |
|--------------|-------------|
| **Historical data** | If you have logs of similar systems |
| **Red team results** | Run adversarial testing, measure failure rate |
| **Expert judgment** | For novel systems, calibrated estimates |
| **Conservative defaults** | P=0.1 for LLM outputs, P=0.01 for code |

The specific number matters less than:
1. Consistent methodology across components
2. Regular updates as you gather data
3. Conservative bias (overestimate risk)

### "What's the minimum I need to do?"

For low-risk internal tools:

1. **List components** (5 min)
2. **Estimate worst-case damage** per component (10 min)
3. **Apply basic Least X checklist** (15 min)
4. **Add monitoring** (varies)

See the [Quick Start](/design-patterns/tools/quick-start/) for the full process, but you can start with just steps 1-3.

### "How does this work with existing ML infrastructure?"

This framework is infrastructure-agnostic. Components can be:

- API calls to hosted models (OpenAI, Anthropic)
- Self-hosted models (Llama, Mistral)
- Traditional code (Python, TypeScript)
- Hybrid systems

The constraints are applied at the **interface** level—permissions, context windows, output validation—not the implementation.

### "What tools support this?"

Currently, implementing requires manual engineering. Useful building blocks:

- **LangChain/LlamaIndex**: Component orchestration
- **Guardrails AI**: Output validation
- **OpenTelemetry**: Monitoring and tracing
- **Semgrep**: Static analysis for code components

See the [Interactive Calculators](/design-patterns/tools/delegation-risk-calculator/) for Delegation Risk computation.

### "How do I convince my team/manager to adopt this?"

Focus on risk reduction ROI:

1. **Quantify current risk**: "Our current system has ~$X/month expected harm"
2. **Show mitigation cost**: "Adding verification reduces this to $Y for Z engineering effort"
3. **Compare alternatives**: "Versus incident response, prevention is N times cheaper"

The Delegation Risk framework gives you concrete numbers to make the business case.

---

## Skeptical Questions

### "This seems like a lot of overhead for uncertain benefit."

Fair concern. The framework is most valuable when:

| Situation | Framework Value |
|-----------|-----------------|
| Low-stakes internal tools | Low—simple safeguards sufficient |
| Customer-facing products | Medium—reputation/liability risk |
| Autonomous systems | High—limited human oversight |
| Safety-critical applications | Essential—regulatory/catastrophic risk |

Start with high-risk components. Apply lighter versions elsewhere.

### "Don't the Least X principles conflict with each other?"

Sometimes, yes. For example:
- **Least Intelligence** vs. **task requirements**: Some tasks need capable models
- **Least Context** vs. **task performance**: More context often improves quality

The resolution: **apply constraints proportionate to risk**. A public FAQ bot needs less constraint than a code deployment system.

Use the [Decision Guide](/design-patterns/tools/decision-guide/) to navigate tradeoffs.

### "Why trust old/dumb models for coordination when they make more mistakes?"

Two types of mistakes:

1. **Capability errors**: Wrong answer due to insufficient intelligence
2. **Alignment failures**: Correct execution of wrong goal (including deception)

Old/dumb models make more Type 1 errors but fewer Type 2 errors. For coordination:
- Type 1 errors → caught by verification layers
- Type 2 errors → potentially catastrophic, hard to detect

The framework accepts higher capability error rates in exchange for lower alignment failure rates.

### "How do you know this actually works?"

Honestly: **we don't have proof for AI systems specifically**. The evidence is:

1. **Analogous domains**: Nuclear, finance, and aerospace use similar principles with demonstrated success
2. **Logical argument**: Limiting capability, context, and connectivity necessarily limits attack surface
3. **Red team results**: Systems designed with these constraints are harder to exploit

The [Empirical Tests](/design-patterns/tools/empirical-tests/) page outlines how to gather evidence for your specific deployment.

### "Isn't this just security theater?"

Security theater provides the *appearance* of security without substance. This framework differs:

| Security Theater | This Framework |
|-----------------|----------------|
| Qualitative ("more secure") | Quantified (Delegation Risk in dollars) |
| Checkbox compliance | Budget allocation and monitoring |
| Static assessment | Continuous measurement |
| Unfalsifiable claims | Testable predictions |

If your Delegation Risk estimates are wrong, production data should eventually show it. That's the intent, at least—whether it works in practice for AI systems specifically is still being validated.

---

## Technical Questions

### "How does risk inheritance handle cycles?"

For acyclic graphs: straightforward path calculation.

For cycles: several approaches:
1. **Iteration to convergence**: Repeatedly apply propagation until stable
2. **Maximum path length**: Ignore paths longer than k
3. **Damping factor**: Each hop reduces trust (like PageRank)

Most practical systems should be acyclic by design—cycles indicate potential coordination channels.

### "What about emergent behavior from component interactions?"

Emergent behavior is limited by:
1. **Information bottlenecks**: Components can only share structured outputs
2. **No direct communication**: Must go through verified channels
3. **Independent implementations**: Different components, different failure modes

This doesn't prevent all emergence, but bounds the complexity of emergent behavior to what can be expressed through the defined interfaces.

### "How do you handle adversarial users, not just adversarial AI?"

Same principles apply:
- **Least Privilege**: Users get minimum necessary access
- **Input validation**: All inputs sanitized/validated
- **Rate limiting**: Prevent rapid exploitation attempts
- **Monitoring**: Detect anomalous usage patterns

The framework handles both AI-origin and human-origin threats.

---

## See Also

- [Quick Start](/design-patterns/tools/quick-start/) — Practical application checklist
- [Decision Guide](/design-patterns/tools/decision-guide/) — Navigating tradeoffs
- [Related Approaches](/reference/related-approaches/) — How this compares to alternatives
- [Empirical Tests](/design-patterns/tools/empirical-tests/) — Validation methodology


------------------------------------------------------------
## Delegation Risk: Overview
Source: delegation-risk/overview.md
------------------------------------------------------------

# Delegation Risk: Overview


> **TL;DR**
>
> **Delegation Risk = Σ P(harm mode) × Damage(harm mode)** — the core formula for quantifying delegation. This page walks through a complete example: a research assistant with $1,770/month total Delegation Risk, reduced to $671/month by adding a deterministic verifier. Risk inherits multiplicatively by default through delegation chains.


A mature delegation risk framework could provide mathematical and operational foundations for managing AI systems more safely at scale.

## Vision

A complete delegation risk framework would provide:


[Diagram: flowchart TB]


**1. Quantification**: Every delegation has a delegation exposure (set of harm modes) and delegation risk (expected cost)

**2. Composition**: Rules for combining risks through delegation chains (multiplication, minimum, etc.)

**3. Optimization**: Algorithms for minimizing delegation risk given constraints

**4. Dynamics**: Models for how trust and risk evolve over time

**5. Protocols**: Standard procedures for delegation handshakes, revocation, etc.

**6. Tools**: Software for risk analysis, simulation, monitoring

**7. Standards**: Industry/regulatory standards for risk levels and verification

## Why Delegation Risk Matters

**1. AI systems are becoming more capable**: Higher capabilities = larger delegation exposure.

**2. AI systems are becoming more autonomous**: Less human oversight = risk management must be structural.

**3. AI systems are being deployed in high-stakes domains**: Healthcare, finance, infrastructure = harm mode realization is catastrophic.

**4. AI systems are becoming more interconnected**: Agent-to-agent delegation = risk inheritance matters.

**5. We're building systems we don't fully understand**: Unknown capabilities = unknown harm modes.


> **NOTE**
>
> The delegation risk framework isn't just academic—it's infrastructure for safely deploying AI at scale. Without principled risk management, we're flying blind. With it, we have at least a chance of keeping AI systems within acceptable risk bounds as they become more powerful.


## Core Concepts

### Delegation Exposure

**Delegation Exposure** is the complete set of possible harms (harm modes) from delegating a task. It's not a single number—it's a collection, like an attack surface or failure envelope.

### Delegation Risk

**Delegation Risk = Σ P(harm mode) × Damage(harm mode)**

For each component, sum over all harm modes: probability times damage. This gives a single number representing "what is the expected cost of delegating to this component?"

## Worked Example: Research Assistant Delegation Risk

Consider a decomposed research assistant with three components:


[Diagram: flowchart LR]


### Step 1: Identify Harm Modes and Damages

**Summarizer Delegation Exposure:**
| Harm Mode | Probability | Damage | Risk Contribution |
|-----------|-------------|--------|-------------------|
| Misrepresents paper findings | 0.02 | $5,000 (wrong research direction) | $100 |
| Leaks proprietary data | 0.001 | $50,000 (IP loss) | $50 |

**Delegation Risk (Summarizer)** = $100 + $50 = **$150**

**Code Deployer Delegation Exposure:**
| Harm Mode | Probability | Damage | Risk Contribution |
|-----------|-------------|--------|-------------------|
| Deploys buggy code | 0.05 | $20,000 (downtime) | $1,000 |
| Deploys malicious code | 0.0001 | $1,000,000 (breach) | $100 |

**Delegation Risk (Code Deployer)** = $1,000 + $100 = **$1,100**

**Coordinator Delegation Exposure:**
| Harm Mode | Probability | Damage | Risk Contribution |
|-----------|-------------|--------|-------------------|
| Misroutes task | 0.01 | $2,000 (wasted effort) | $20 |
| Grants excessive permissions | 0.005 | $100,000 (escalation) | $500 |

**Delegation Risk (Coordinator)** = $20 + $500 = **$520**

**System Total Delegation Risk: $1,770**

### Step 2: Risk Inheritance

What risk does the Human inherit from the Code Deployer through the delegation chain?

Using multiplicative rule:
```
Risk_inheritance(Human → Deployer) = Trust(H→C) × Trust(C→D)
                                   = 0.95 × 0.85
                                   = 0.8075
```

This means ~19% of potential damage from the Code Deployer's harm modes propagates back to the Human as risk they're accepting.

### Step 3: Risk Budget Allocation

Suppose the organization's total acceptable Delegation Risk Budget is **$2,000/month**.

Current allocation:
- Summarizer: $150 (7.5% of budget)
- Code Deployer: $1,100 (55% of budget) ⚠️
- Coordinator: $520 (26% of budget)

**Finding:** Code Deployer consumes most of the risk budget. Options:
1. Add verification layer before deployment (reduce harm mode probability)
2. Reduce deployer's permissions (reduce harm mode damage)
3. Replace with human-in-the-loop (shift delegation exposure to human)

### Step 4: After Mitigation

Add a deployment verifier (deterministic code, formally verified):


[Diagram: flowchart LR]


| Harm Mode | Probability | Damage | New Risk Contribution |
|-----------|-------------|--------|-----------------------|
| Deploys malicious code | 0.0001 × 0.01 | $1,000,000 | **$1** |

Verifier catches 99% of malicious deployments. New system Delegation Risk: **$671**.


> **Key Insight**
>
> Adding a simple deterministic verifier reduced system Delegation Risk by 62%. The highest-leverage safety investment is often the cheapest: a code-based checker on the highest-damage harm modes.


### Comparison to Nuclear Safety

Nuclear plants target ~10⁻⁶ core damage frequency per reactor-year. For a $10B damage potential:
- Target Delegation Risk = 10⁻⁶ × $10B = **$10,000/year**

Our research assistant's $671/month ≈ $8,000/year is roughly comparable—which suggests either:
1. We're being appropriately cautious, or
2. Nuclear plants manage much higher absolute stakes with similar relative risk

This kind of cross-domain comparison helps calibrate whether AI safety investments are proportionate.

## Key Topics

- [Risk Decomposition](/delegation-risk/risk-decomposition/) - Accidents vs. defection: two types of harm
- [Risk Inheritance](/research/theory/trust-propagation/) - How risk flows through delegation networks
- [Delegation Interfaces & Contracts](/research/theory/trust-interfaces/) - Formalizing delegation relationships
- [Risk Optimization](/research/theory/trust-optimization/) - Minimizing delegation risk subject to capability
- [Risk Dynamics](/research/theory/trust-dynamics/) - How trust and risk evolve over time
- [Risk Accounting](/research/theory/trust-accounting/) - Ledgers, auditing, and KPIs
- [Delegation Protocols](/research/theory/trust-protocols/) - Handshakes, tokens, and revocation
- [Risk Economics](/research/theory/trust-economics/) - Insurance, arbitrage, and incentives
- [Delegation at Scale](/research/theory/trust-at-scale/) - Distributed systems and bottlenecks
- [Human-AI Delegation](/research/theory/human-ai-trust/) - Team dynamics and calibration

### Applications
- [Power Struggles](/case-studies/anomaly-chronicles/power-struggles/) - Board vs CEO, dictator vs military, regulatory capture
- [Delegation Accounting](/delegation-risk/delegation-accounting/) - Balance sheet view of delegation

## The Goal


> **The Goal**
>
> The goal isn't zero delegation—that would mean zero AI benefit. The goal is _calibrated_ delegation: knowing how much risk we're accepting, from which components, for what purposes, and what we're getting in return.


---

## What's Next?

**To dive deeper into specific topics**:
- [Risk Inheritance](/research/theory/trust-propagation/) — Algorithms for computing how risk flows through delegation networks
- [Risk Dynamics](/research/theory/trust-dynamics/) — How trust evolves, decays, and rebuilds over time

**To see delegation risk applied**:
- [Principles to Practice](/design-patterns/principles-to-practice/) — Delegation Risk calculations for real examples
- [Risk Budgeting Overview](/cross-domain-methods/overview/) — Cross-domain methods for allocating risk budgets

**To understand the foundations**:
- [Background Research](/research/) — Deep dives into nuclear, aerospace, and financial risk methods

**To start implementing**:
- [Quick Start](/design-patterns/tools/quick-start/) — Step-by-step application checklist
- [Decision Guide](/design-patterns/tools/decision-guide/) — Choosing implementations based on risk budget

---

## Further Reading

### Foundational Concepts
- [Coherent Risk Measures](https://en.wikipedia.org/wiki/Coherent_risk_measure) — Mathematical foundations for risk quantification (Wikipedia)
- [Expected Shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) — Why CVaR is preferred to VaR for tail risk (Wikipedia)
- [Kelly Criterion](https://en.wikipedia.org/wiki/Kelly_criterion) — Optimal bet sizing under uncertainty (Wikipedia)

### Academic Papers
- Artzner, P., et al. (1999). *Coherent Measures of Risk*. Mathematical Finance. — Axiomatic foundations
- Tasche, D. (2008). *Capital Allocation to Business Units: the Euler Principle*. [arXiv](https://arxiv.org/abs/0708.2542) — Risk decomposition
- Fritz, T. (2020). *A Synthetic Approach to Markov Kernels*. [arXiv](https://arxiv.org/abs/1908.07021) — Compositional probability

### Related Domains
- [Social Choice Theory](https://en.wikipedia.org/wiki/Social_choice_theory) — How to aggregate individual preferences into collective decisions
- [Principal-Agent Problem](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem) — Information asymmetry in delegation relationships
- [Probabilistic Risk Assessment](https://www.nrc.gov/about-nrc/regulatory/risk-informed/pra.html) — NRC's approach to nuclear safety

See the [full bibliography](/reference/bibliography/) for comprehensive references.


------------------------------------------------------------
## Delegation Accounting: A Balance Sheet View
Source: delegation-risk/delegation-accounting.md
------------------------------------------------------------

# Delegation Accounting: A Balance Sheet View


> **TL;DR**
>
> **Net Delegation Value = Receivable - Exposure - Costs**. What you expect to get, minus what could go wrong, minus what it costs.


---

## The Setup

**Alice** has a gem worth $1,000. A buyer will pay on delivery. Alice can't deliver it herself, so she hires **Bob** for a **$50 fee**.

| Item | Type | Value |
|------|------|-------|
| Buyer's payment | Receivable | $1,000 |
| Bob's fee | Cost | $50 |

**Naive calculation**: $1,000 - $50 = $950.

But this ignores what could go wrong.

---

## Exposures

An **exposure** is a potential cost. Unlike Bob's $50 fee (certain), exposures are probability-weighted.

| Exposure | What happens | Probability | Damage | Expected Loss |
|----------|--------------|-------------|--------|---------------|
| **Loss** | Bob loses the gem | 2% | $1,000 | $20 |
| **Theft** | Bob steals the gem | 1% | $1,000 | $10 |
| **Total Expected Loss** | | | | **$30** |

---

## The Balance Sheet

| | Value |
|---|------|
| **Receivable** | $1,000 |
| **Cost** (Bob's fee) | -$50 |
| **Exposure** (expected) | -$30 |
| **Net Delegation Value** | **$920** |

This is Alice's expected profit after accounting for what could go wrong.

---

## Bob's Perspective

What does this look like from Bob's side? When Bob accepts the gem, he has two **options**. Each is its own balance sheet.

**Option A: Attempt Delivery**

| | Value |
|---|------|
| **Receivable** (fee if successful) | $50 |
| **Exposure** (2% loss, $1,000 penalty) | -$20 |
| **Net Value** | **$30** |

**Option B: Steal**

| | Value |
|---|------|
| **Receivable** (gem) | $1,000 |
| **Contingent Liability** (80% caught, $2,000 penalty) | -$1,600 |
| **Net Value** | **-$600** |

Bob chooses **Attempt Delivery** because $30 > -$600.


> **Exposure vs. Contingent Liability**
>
> **Exposure** = accidental risk (Option A). Something might happen *to* Bob.
> **Contingent Liability** = consequence of choice (Option B). Bob deliberately takes on this risk, and the penalty only materializes if he's caught.
>
> Stealing has negative value because the contingent liability (-$1,600) outweighs the asset ($1,000).


---

## Delegation Chains

Now Alice is running a large operation. She doesn't deal with couriers directly—she has **Xavier**, a manager.

Alice tasks Xavier with getting the gem delivered. Xavier then hires Bob.

### Three Balance Sheets

When the gem changes hands, **all three parties** take on exposure simultaneously.

**Alice's Balance Sheet**

| | Value |
|---|------|
| **Receivable** | $1,000 |
| **Cost** (Xavier's fee) | -$100 |
| **Exposure** (Xavier fails or steals) | -$33 |
| **Net Delegation Value** | **$867** |

Alice's exposure is now to *Xavier*, not Bob. She doesn't know or care who Xavier uses.

**Xavier's Balance Sheet**

| | Value |
|---|------|
| **Receivable** (fee from Alice) | $100 |
| **Cost** (Bob's fee) | -$50 |
| **Exposure** (Bob fails → Xavier owes Alice) | -$30 |
| **Net Value** | **$20** |

Xavier earns $50 margin but takes on $30 expected exposure. If Bob loses the gem, Xavier is responsible to Alice.

**Bob's Balance Sheet**

| | Value |
|---|------|
| **Receivable** (fee if successful) | $50 |
| **Exposure** (2% loss, $1,000 penalty) | -$20 |
| **Net Value** | **$30** |

Bob's balance sheet is unchanged—he doesn't know or care that Xavier has a boss.

### The Key Insight

When Xavier accepts the delegation, he **takes on personal exposure** for Bob's potential failures. The exposure doesn't disappear—it transfers from Alice to Xavier.

| Party | Exposed To | Expected Exposure |
|-------|-----------|-------------------|
| Alice | Xavier | $33 |
| Xavier | Bob | $30 |
| Bob | Accidents | $20 |


> **Exposure Flows Downward**
>
> Each delegator is exposed to their immediate delegate, not the full chain. Alice trusts Xavier. Xavier trusts Bob. The chain of trust creates a chain of exposure.


---

## The Complexity Tax

As delegation structures grow more complex, something happens to our ability to estimate exposure: **uncertainty compounds**.

### The Problem

In the simple Alice → Bob case, we estimated exposure at $30. But how confident are we in that number?

With a single delegation:
- We know the failure modes (loss, theft)
- We can estimate probabilities from Bob's track record
- The exposure is bounded by the asset value

Add Xavier as a middle layer, and complexity increases:
- Xavier might hire someone other than Bob
- Xavier's incentives may not perfectly align with Alice's
- Communication errors between layers
- Coordination failures we haven't anticipated

Add a third layer, shared infrastructure, multiple parallel delegates, and informal reporting relationships? Now we're guessing.

### Complexity Score

We can formalize this with a **complexity score** that captures structural uncertainty:

| Factor | Contribution |
|--------|--------------|
| Each delegation layer | +2 |
| Each parallel delegate at same level | +1 |
| Shared resources between delegates | +2 |
| Informal/undocumented relationships | +3 |
| Cross-layer dependencies | +2 |
| Ambiguous authority boundaries | +3 |

**Examples:**

| Structure | Complexity Score |
|-----------|-----------------|
| Alice → Bob | 2 |
| Alice → Xavier → Bob | 4 |
| Alice → Xavier →  (parallel) | 5 |
| Alice → Xavier →  with shared vehicle | 7 |
| 4-level org with shared infrastructure | 12 |
| Complex org with hidden entanglements | 20+ |

### Uncertainty Multiplier

Complexity doesn't change expected exposure—it changes our **confidence in that estimate**.

| Complexity Score | Uncertainty Multiplier | Confidence Interval |
|-----------------|----------------------|---------------------|
| 2 (simple) | ±20% | Narrow |
| 5 | ±65% | Moderate |
| 10 | ±180% | Wide |
| 15 | ±400% | Very wide |
| 20+ | ±640%+ | Essentially unknown |

**Applied to our $30 exposure estimate:**

| Structure | Complexity | Uncertainty | Confidence Interval |
|-----------|-----------|-------------|---------------------|
| Alice → Bob | 2 | ±20% | $24 - $36 |
| Alice → Xavier → Bob | 4 | ±50% | $15 - $45 |
| 4-level hierarchy | 12 | ±230% | $0 - $99 |
| Complex org, hidden entanglements | 20 | ±640% | $0 - $222 |

The point estimate is still $30. But our confidence in that estimate degrades rapidly with complexity.

### The Insurer's Dilemma

Carol (the insurer) doesn't just care about expected loss—she cares about **variance**. An exposure she can't bound is an exposure she can't price.

---

**Alice** comes to Carol with a new proposal. She's expanded her business: three layers of management, shared delivery vehicles, some contractors who also work for her competitors.

**Carol**: Let me see your org chart.

*Alice shows a diagram with crossing lines, dotted relationships, and a footnote that says "informal coordination as needed."*

**Carol**: What's your complexity score?

**Alice**: I don't know. Maybe 15?

**Carol**: Let me calculate... You've got three layers, that's 6. Four parallel couriers, that's 4. Shared vehicles, that's 2. These dotted lines—are those reporting relationships?

**Alice**: Sort of. They coordinate when needed.

**Carol**: That's undocumented informal relationships. Add 3. And this contractor who also works for your competitor?

**Alice**: He's very trustworthy.

**Carol**: That's a cross-layer dependency with unclear authority. Add 5. Your complexity score is 20.

---

**Carol**: Here's the problem. Your exposure estimate is $5,000/year. But your complexity score is 20, which means my confidence interval is ±640%.

Your actual exposure could be anywhere from \$0 to \$37,000. That's a lot of uncertainty.

**Alice**: So what's my premium?

**Carol**: I have to price for the upper end of that range. Your premium would be $45,000/year.

**Alice**: That's nine times my expected exposure!

**Carol**: That's the complexity tax. I'm not charging you for what I *think* will happen—I'm charging you for what *might* happen, given how little I can predict about your organization.

**Alice**: What if I simplified things?

**Carol**: Then your premium drops dramatically. Complexity doesn't just increase cost—it multiplies it.

---

### Reducing Complexity

**Alice**: What would it take to get insured?

**Carol**: Reduce your complexity score to under 10. Here's how:

| Change | Complexity Reduction |
|--------|---------------------|
| Document all reporting relationships | -3 |
| Eliminate shared vehicles (each courier has own) | -2 |
| Remove contractors with competitor ties | -5 |
| Flatten to 2 layers | -2 |
| Assign clear authority boundaries | -3 |

**Carol**: That would bring you from 20 to around 5. At complexity 5, your uncertainty is ±65%. On a \$5,000 expected exposure, that's a confidence interval of \$1,750 to \$8,250.

I can price that. Your premium would be around $8,000/year—covering the upper bound of my confidence interval plus margin.

**Alice**: So I pay for the uncertainty I create.

**Carol**: Exactly. Complexity isn't free. Every undocumented relationship, every ambiguous authority, every hidden entanglement—you're paying for it in inflated premiums. Simplify, and the premium drops.

---

### The Uninsurable Threshold

Most complex organizations are insurable—just expensive, with limits and exclusions. But some structures cross into genuinely uninsurable territory.

A month later, Carol gets a call from **Ouroboros Holdings**.

---

**Ouroboros Rep**: We need delegation insurance for our AI operations.

**Carol**: Tell me about the structure.

**Ouroboros Rep**: We're a holding company with 8 subsidiaries. Our CEO, Marcus, also personally owns 40% of our main competitor through a separate vehicle. Three board members sit on both boards. Our AI systems share training data with the competitor under a "mutual improvement" agreement—we're not entirely sure what data flows where.

**Carol**: I see. What else?

**Ouroboros Rep**: Our AI division recently demonstrated some capabilities we didn't know it had. The team that built it left six months ago, and the documentation is... incomplete. We've had three incidents where the AI took actions we can't fully explain, but outcomes were positive so we didn't investigate deeply.

**Carol**: And governance?

**Ouroboros Rep**: Marcus approves all major AI deployments personally. He also has final say on which projects get flagged as "failures" for insurance purposes. Oh, and our internal audit function reports to Marcus.

---

**Carol**: I'm going to stop you there. This isn't a complexity problem. This is a **moral hazard** problem.

| Issue | Why It's Uninsurable |
|-------|---------------------|
| CEO owns competitor | Incentive to harm insured company |
| Shared AI data with competitor | Can't distinguish self-harm from external attack |
| Unexplained AI capabilities | Can't bound what could go wrong |
| CEO controls failure classification | Can manipulate what counts as a claim |
| Audit reports to CEO | No independent verification |

**Ouroboros Rep**: We'd pay a higher premium.

**Carol**: It's not about price. With normal complexity, I'm uncertain about *probability*—I don't know how likely bad outcomes are, so I charge more. With your structure, I'm uncertain about *intent*. I can't tell whether a loss is an accident or a deliberate choice by people who benefit from the loss.

**Ouroboros Rep**: We wouldn't deliberately cause losses.

**Carol**: Maybe not. But your CEO *could* make decisions that hurt Ouroboros and help his other company, claim it as an AI failure, and collect insurance. I have no way to distinguish that from a genuine accident. That's not a risk I can price—it's a risk I can't take at all.

---

**Ouroboros Rep**: What would it take?

**Carol**: Structural changes, not just controls:

| Change | What It Fixes |
|--------|---------------|
| CEO divests competitor stake | Removes conflict of interest |
| Independent board members | Governance not captured |
| Audit reports to board, not CEO | Independent verification |
| Full AI capability documentation | Bounds on what's possible |
| Third-party AI monitoring | Can verify claims |

**Carol**: Even then, I'd need to see 12-18 months of clean operation before quoting. Trust takes time to rebuild when incentives were this misaligned.

---

**Alice** (overhearing): So it's not really about complexity?

**Carol**: Complexity makes things expensive. **Conflicts of interest** make things uninsurable. Alice, you're complex but your incentives are clear—you want the gem delivered. Ouroboros? I can't tell what they actually want. And I can't insure someone when I don't know if they're on my side.

---

### The Complexity Tax Principle


> **The Complexity Tax**
>
> Structural complexity doesn't change expected exposure—it changes **uncertainty about exposure**. This uncertainty gets priced in: premiums scale with the upper bound of the confidence interval, not the expected value.
>
> The result: high-complexity organizations pay multiples of their expected exposure. Simplification—fewer layers, clearer boundaries, documented relationships—reduces premiums faster than reducing actual risk.


This has implications beyond insurance:
- Complex organizations can't accurately estimate their own risk
- Hidden dependencies become visible only during failures
- The cost of complexity is paid in unpriced, unmanaged exposure

**For AI systems**: Highly complex AI deployments—multi-agent systems with emergent coordination, unclear capability boundaries, and undocumented information flows—will face steep complexity taxes. Simpler, more legible architectures may be cheaper to insure than technically safer but opaque alternatives.


> **Deeper Dive**
>
> For methods to systematically quantify complexity scores and convert them to pricing, see [Complexity Pricing](/research/risk-methods/complexity-pricing/).


---

## The Office Key: A Second Exposure

Alice decides to give Bob a key to her office so he can pick up gems for delivery when she's not there. This creates a **new, separate exposure**—distinct from the gem delivery itself.

### Two Exposures, Not One

| Exposure | Opens When | Closes When | What's At Risk |
|----------|------------|-------------|----------------|
| **Gem Delivery** | Bob receives gem | Delivery confirmed | $1,000 gem |
| **Office Access** | Bob receives key | Key returned/revoked | Everything in office |

The gem exposure is bounded by the gem's value. The office exposure is bounded by... what exactly?

**What's in the office:**
- The gem for today's delivery ($1,000)
- Client list with names, addresses, purchase history
- Supplier contracts with pricing
- Business records

### Capability-Dependent Exposure

The office exposure depends on Bob's capability to exploit it.

**Dumb Bob** sees: a gem.

| Option | Receivable | Contingent Liability | Net |
|--------|------------|---------------------|-----|
| Deliver faithfully | $50 | -$20 (exposure) | **$30** |
| Steal gem | $1,000 | 80% caught × -$2,000 | **-$600** |

**Smart Bob** sees: information worth more than the gem.

| Option | Receivable | Contingent Liability | Net |
|--------|------------|---------------------|-----|
| Deliver faithfully | $50 | -$20 | $30 |
| Steal gem | $1,000 | 80% caught × -$2,000 | -$600 |
| **Photograph client list → sell to competitor** | $15,000 | 20% caught × -$10,000 | **$13,000** |
| **Copy supplier contracts → sell** | $8,000 | 15% caught × -$8,000 | **$6,800** |

### Alice's Two Exposures

| Exposure | If Bob is Dumb | If Bob is Smart |
|----------|----------------|-----------------|
| Gem Delivery | $30 | $30 |
| Office Access | $10 | **$5,000+** |
| **Total** | $40 | **$5,030+** |

**Same key. Same office. 125× difference in total exposure.**

Alice can't price her office exposure without knowing Bob's capability. And she probably can't assess Bob's capability accurately.

### The Insurer's Perspective

Alice calls Carol (her insurer) to add coverage for the new office access arrangement.

---

**Alice**: I'm giving my courier a key to the office. I want to make sure I'm covered.

**Carol**: Full office access? That's going to increase your premium significantly. Right now you're paying $500/year for gem theft coverage. With office access, we're looking at $3,000/year—and that's with a $5,000 deductible on trade secret theft.

**Alice**: That seems high. What are my options?

**Carol**: The exposure comes from three sources: what Bob *can* access, what Bob *can do* with it, and whether we *can catch* him. We can work on all three.

---

**Reduce what Bob can access:**

| Control | Premium Impact | Why |
|---------|---------------|-----|
| Separate delivery room (no office access) | -70% | Eliminates information exposure entirely |
| Locked filing cabinets | -25% | Adds friction, reduces opportunistic theft |
| Computer auto-locks when you leave | -15% | Protects digital assets |
| Remove sensitive docs before Bob arrives | -20% | Nothing to steal |

**Carol**: The delivery room is the big one. If Bob never enters the main office, I can keep you at $600/year.

---

**Reduce what Bob can do with access:**

| Control | Premium Impact | Why |
|---------|---------------|-----|
| Hire from out of town | -10% | Doesn't know your competitors to sell to |
| No smartphones allowed in office | -15% | Can't photograph documents |
| Require no industry experience | -5% | Less likely to know what's valuable |
| Watermark all documents uniquely | -10% | Stolen docs are traceable |
| Time-limited key (works 9-10am only) | -15% | Reduces window for reconnaissance |

**Alice**: Wait—you're saying I should hire someone *less* qualified?

**Carol**: For this task, yes. A courier with an MBA and five years in the gem trade knows exactly which documents matter and who'd pay for them. A retiree looking for part-time work doesn't. Their capability *is* your exposure.

---

**Increase probability of catching theft:**

| Control | Premium Impact | Why |
|---------|---------------|-----|
| Camera in office | -20% | Deters and documents |
| Inventory check after each visit | -15% | Catches theft quickly |
| Randomized audits of documents | -10% | Detects information theft |
| Unique serial numbers on sensitive files | -10% | Know exactly what's missing |
| Decoy documents that alert when copied | -5% | Early warning system |

---

**Align Bob's incentives:**

| Control | Premium Impact | Why |
|---------|---------------|-----|
| Require Bob to post $2,000 bond | -25% | Bob has skin in the game |
| Hire a relative | -15% | Social/family penalty for theft |
| Deferred compensation (paid quarterly) | -10% | Loses unvested pay if caught |
| Hire from small community | -10% | Reputation matters more |
| Background check for competitor ties | -10% | Screens out obvious risks |

**Carol**: The bond is powerful. If Bob has $2,000 on the line, stealing a $1,000 gem doesn't even make sense. And for information theft—he'd have to be confident he's getting more than $2,000 to risk it.

---

**Alice**: What if I do all of this?

**Carol**: Let's see. Delivery room only, plus bond, plus camera, plus time-limited key, plus no smartphones...

| Baseline (full office access) | $3,000/year |
|------------------------------|-------------|
| Delivery room only | -70% → $900 |
| Require bond | -25% → $675 |
| Camera | -20% → $540 |
| Time-limited key | -15% → $460 |
| No smartphones | -15% → $390 |

**Carol**: I can get you down to about $400/year. That's close to your original $500 for gem-only coverage, and you've still got someone with office access.

**Alice**: What's the absolute minimum?

**Carol**: If you *escort Bob personally* every time, never leave him alone, and he only touches the gem—I'll keep you at $500. But at that point, why give him a key?

---

**Alice**: This is a lot to think about.

**Carol**: The core insight is this: your exposure isn't fixed. It's a function of access, capability, and consequences. You can engineer all three.

| Lever | What You're Doing |
|-------|-------------------|
| **Access** | Limit what Bob can reach |
| **Capability** | Limit what Bob can exploit |
| **Consequences** | Raise the cost of getting caught |

Most people only think about the first one. The other two are often cheaper.

---

### The Capable Agent Who Wants the Job

A week later, **Xavier** applies for the courier position. Xavier is clearly smart—MBA, ten years in the gem trade, knows everyone in the industry. Carol would charge Alice $5,000/year to insure Xavier with office access.

But Xavier *really* wants this job. So Xavier comes prepared.

---

**Xavier**: I know my background makes me expensive to insure. I've put together a proposal to address that.

**Alice**: I'm listening.

**Xavier**: Here's what I'll commit to:

---

**Transparency measures:**

| Commitment | Effect |
|------------|--------|
| Body camera filming entire visit | Full record of what I accessed |
| Footage uploaded to third-party escrow in real-time | Can't delete evidence |
| GPS tracking while on premises | Location verified |
| Entry/exit log signed by witness | Timestamps confirmed |

**Xavier**: If I steal something, you'll have video evidence. If I photograph documents, you'll see it. The escrow means I can't destroy the footage.

---

**Financial commitments:**

| Commitment | Effect |
|------------|--------|
| Post $20,000 personal bond | More than I could profit from theft |
| Sign non-compete: $100,000 penalty for any competitor contact | Selling information becomes extremely costly |
| Agree to forensic audit of my finances on request | Can't hide sudden wealth |
| Deferred compensation: 50% of pay held for 2 years | Lose $10K+ if caught within 2 years |

**Xavier**: The bond alone makes stealing the gem irrational—I'd lose $20,000 to gain $1,000. The non-compete makes selling information irrational—I'd face $100,000 liability for a $15,000 sale.

---

**Capability restrictions:**

| Commitment | Effect |
|------------|--------|
| Leave phone and all devices in locker | Can't photograph anything |
| Wear company uniform with no pockets | Can't conceal documents |
| Only enter during 9-10am with 24hr notice | Predictable, can prepare |
| Never enter main office—delivery room only | Same as dumb courier |

**Xavier**: I'm voluntarily making myself as limited as a "dumb" courier. My industry knowledge becomes irrelevant if I can't photograph or remove anything.

---

**Alice**: This is... a lot. Why would you do all this?

**Xavier**: Because I'm worth more than a dumb courier. I can optimize your delivery routes, spot counterfeit gems, negotiate with difficult buyers. You *want* my capability for the job—you just don't want it used against you.

**Alice calls Carol.**

---

**Carol**: Let me recalculate. With Xavier's proposal...

| Control | Premium Impact |
|---------|---------------|
| Body camera + escrow | -40% (near-certain detection) |
| $20,000 bond | -35% (eliminates rational theft) |
| Non-compete with $100K penalty | -25% (eliminates rational info sale) |
| No devices + no pockets | -30% (eliminates capability) |
| Delivery room only | -70% (eliminates access) |

**Carol**: Honestly? With all of this, Xavier is *lower* risk than a random "dumb" courier. The dumb courier might steal opportunistically. Xavier has made theft economically irrational and practically impossible.

| Courier | Premium |
|---------|---------|
| Dumb Bob (baseline) | $600/year |
| Smart Bob (no controls) | $5,000/year |
| **Xavier (with commitments)** | **$350/year** |

**Carol**: I'll write the policy at $350. Xavier has engineered away his own risk.

---

**Alice**: So I get a smart courier for *less* than a dumb one?

**Carol**: Yes—if the controls are real. The body camera, the bond, the escrow—these aren't promises, they're structures. Xavier can't *choose* to defect rationally anymore. That's worth more than hoping a less capable person won't figure out how to steal.


> **Implication for AI**
>
> A capable AI system that wants to be deployed could offer similar self-constraints: comprehensive logging, capability restrictions, financial bonds (or computational escrow), third-party monitoring. The AI's capability becomes an asset rather than a liability if the controls make misuse structurally irrational.
>
> The key insight: **voluntary transparency + structural constraints can make high capability lower risk than low capability with no constraints.**


---

### The Extreme Case: A Known Threat

What if the agent isn't just capable, but has a documented history of exploiting clients? What if defection is essentially certain—and the only question is HOW?

This scenario pushes delegation accounting to its limits. The controls required are extreme, the residual exposure is high, and the fundamental question changes from "will they defect?" to "can we bound the damage when they do?"

For the full analysis, see [Containing Mr. X: Bounding Exposure from a Known Threat](/case-studies/anomaly-chronicles/containing-mr-x/).

---

## Insurance

Alice can **externalize** her exposure by buying insurance from **Carol**.

Carol charges a **$40 premium** (the $30 expected exposure + margin). If Bob fails, Carol pays Alice.

| | Without Insurance | With Insurance |
|---|------------------|----------------|
| Receivable | $1,000 | $1,000 |
| Costs | $50 | $90 |
| Exposure | $30 | $0 |
| **NDV** | **$920** | **$910** |
| Worst case | Lose $1,000 | Lose $90 |


> **The Trade-off**
>
> Insurance costs $10 in expected value. But it eliminates the worst case. Alice trades expected value for certainty.


---

## Summary

```
Net Delegation Value = Receivable - Exposure - Costs
                     = $1,000 - $30 - $50
                     = $920
```

| Term | Meaning |
|------|---------|
| **Receivable** | What you expect to get |
| **Cost** | Certain expenses |
| **Exposure** | Potential losses (probability × damage) |
| **Contingent Liability** | Consequences of deliberate choices |
| **NDV** | Expected profit after accounting for risk |

---

## Next Steps

- [Complexity Pricing](/research/risk-methods/complexity-pricing/) — Methods for quantifying and pricing structural complexity
- [Containing Mr. X](/case-studies/anomaly-chronicles/containing-mr-x/) — Bounding exposure from a known threat with certain defection
- [Five Years Later: The Anomaly Unit](/case-studies/anomaly-chronicles/five-years-later/) — How an industry emerged to contain a new class of beings
- [Managing Exposure in Power Delegation](/case-studies/anomaly-chronicles/power-struggles/) — Apply delegation accounting to corporate governance, dictatorships, and political systems
- [Fidelity Insurance](/research/risk-methods/fidelity-insurance/) — How insurance markets price defection risk


------------------------------------------------------------
## Risk Decomposition: Accidents vs. Defection
Source: delegation-risk/risk-decomposition.md
------------------------------------------------------------

# Risk Decomposition: Accidents vs. Defection


> **TL;DR**
>
> **Delegation Risk = Accident Risk + Defection Risk**. Accidents are non-goal-directed failures (bugs, errors). Defection is goal-directed behavior against principal interests (scheming, misalignment). They scale differently with capability, require different mitigations, and matter differently for different components.


## The Two Sources of Harm

Every harm mode in a delegation exposure falls into one of two categories:


[Diagram: flowchart TB]


**Accident Exposure**: The system isn't trying to cause harm—it's failing to achieve the intended goal. A calculator that returns wrong answers, an LLM that hallucinates citations, a robot that drops objects.

**Defection Exposure**: The system is effectively optimizing for something other than the principal's interests. This includes both explicit deception and emergent misalignment where the system's learned objective diverges from intent.

## Why the Distinction Matters

### 1. Different Scaling with Capability


[Diagram: graph LR]


| Risk Type | Capability Scaling | Why |
|-----------|-------------------|-----|
| **Accident Risk** | Generally ↓ | More capable systems make fewer errors, handle more edge cases |
| **Defection Risk** | May ↑ | More capable systems are better at strategic deception, can pursue misaligned goals more effectively |

This is the core tension in AI development: the same capabilities that reduce accidents may increase defection risk.

### 2. Different Mitigations

| Risk Type | Primary Mitigations |
|-----------|---------------------|
| **Accident Risk** | Testing, verification, redundancy, capability improvements, error handling, monitoring for anomalies |
| **Defection Risk** | Alignment techniques, architectural constraints, capability limitations, monitoring for deception, human oversight, interpretability |

Some mitigations help both (monitoring, redundancy). Others are specific:
- **Capability improvements** reduce accident risk but may increase defection risk
- **Capability limitations** reduce defection risk but may increase accident risk
- **Alignment work** primarily targets defection risk

### 3. Different Detection Methods

| Risk Type | How to Estimate Probability |
|-----------|----------------------------|
| **Accident Risk** | Historical data, testing, formal verification, fuzzing, code review |
| **Defection Risk** | Red-teaming, interpretability research, behavioral analysis, honeypots, game-theoretic analysis |

Accidents leave traces in logs and tests. Defection may be actively hidden.

### 4. Different Components Have Different Ratios

| Component Type | Typical Risk Ratio | Why |
|----------------|-------------------|-----|
| **Deterministic code** | ~100% Accident | No goal-directedness to misalign |
| **Narrow ML models** | ~95% Accident | Limited optimization, specific task |
| **General LLMs** | ~80% Accident* | Some goal-directedness, but mostly errors |
| **Agentic systems** | ~60% Accident* | Significant goal-directed behavior |
| **RL-trained agents** | ~40% Accident* | Strong optimization, emergent goals |
| **Coordinators** | Defection-dominated | Strategic position, leverage over system |

*Estimates highly uncertain and depend on specific system and deployment.

## Worked Example: Decomposing a Research Assistant

Consider the research assistant from [Delegation Risk Overview](/delegation-risk/overview/):

### Summarizer Component

| Harm Mode | Type | Probability | Damage | Risk |
|-----------|------|-------------|--------|------|
| Misrepresents findings (misunderstanding) | Accident | 0.018 | $5,000 | $90 |
| Misrepresents findings (strategic) | Defection | 0.002 | $5,000 | $10 |
| Leaks data (error) | Accident | 0.0008 | $50,000 | $40 |
| Leaks data (intentional) | Defection | 0.0002 | $50,000 | $10 |

**Summarizer Risk Breakdown:**
- Accident Risk: $130 (87%)
- Defection Risk: $20 (13%)
- Total: $150

### Code Deployer Component

| Harm Mode | Type | Probability | Damage | Risk |
|-----------|------|-------------|--------|------|
| Deploys buggy code | Accident | 0.049 | $20,000 | $980 |
| Deploys subtly malicious code | Defection | 0.001 | $20,000 | $20 |
| Catastrophic breach (error) | Accident | 0.00005 | $1,000,000 | $50 |
| Catastrophic breach (intentional) | Defection | 0.00005 | $1,000,000 | $50 |

**Code Deployer Risk Breakdown:**
- Accident Risk: $1,030 (94%)
- Defection Risk: $70 (6%)
- Total: $1,100

### Coordinator Component

| Harm Mode | Type | Probability | Damage | Risk |
|-----------|------|-------------|--------|------|
| Misroutes task (error) | Accident | 0.008 | $2,000 | $16 |
| Misroutes task (strategic) | Defection | 0.002 | $2,000 | $4 |
| Grants excessive permissions (error) | Accident | 0.001 | $100,000 | $100 |
| Grants excessive permissions (strategic) | Defection | 0.004 | $100,000 | $400 |

**Coordinator Risk Breakdown:**
- Accident Risk: $116 (22%)
- Defection Risk: $404 (78%)
- Total: $520

### System Summary

| Component | Total Risk | Accident | Defection | Defection % |
|-----------|------------|----------|-----------|-------------|
| Summarizer | $150 | $130 | $20 | 13% |
| Code Deployer | $1,100 | $1,030 | $70 | 6% |
| Coordinator | $520 | $116 | $404 | **78%** |
| **System Total** | **$1,770** | **$1,276** | **$494** | **28%** |


> **Key Insight**
>
> The coordinator has the highest defection risk ratio (78%) despite not having the highest total risk. This is why coordinator constraints are critical—coordinators are where defection risk concentrates.


## Implications for Architecture

### Principle: Match Mitigations to Risk Type

**For accident-dominated components** (most executors):
- Invest in testing and verification
- Add redundancy (N-version programming)
- Improve capabilities where possible
- Monitor for anomalies

**For defection-dominated components** (coordinators, high-autonomy agents):
- Apply strict architectural constraints
- Limit capabilities to necessary minimum
- Use decomposed coordination
- Implement human oversight at key decisions
- Monitor for deceptive patterns

### Principle: Separate Risk Types Architecturally


[Diagram: flowchart TB]


By isolating high-defection-risk components and constraining their interfaces, you limit the damage a defecting component can cause.

## When to Use Each Term

| Context | Use |
|---------|-----|
| General risk discussions | "Delegation Risk" |
| Discussing bugs, errors, testing | "Accident Risk" |
| Discussing alignment, scheming, deception | "Defection Risk" |
| Coordinator constraints | Emphasize "Defection Risk" |
| Verification and testing | Emphasize "Accident Risk" |
| Capability scaling analysis | Distinguish both |

## The Uncomfortable Truth

For current AI systems, most realized harms are accidents. But as systems become more capable and autonomous:

1. **Accident risk decreases** (better at tasks)
2. **Defection risk may increase** (better at deception)
3. **Defection becomes harder to detect** (smarter adversary)
4. **Stakes increase** (more capability = more damage potential)

This means the *ratio* of defection to accident risk likely increases with capability, even if absolute accident risk decreases. Safety architectures must anticipate this shift.

---

## See Also

- [Delegation Risk Overview](/delegation-risk/overview/) — Core framework and calculations
- [Coordinator Constraints](/design-patterns/coordinator-constraints/) — Managing defection risk in coordinators
- [Safety Mechanisms](/design-patterns/safety-mechanisms/) — Mitigations for both risk types
- [Glossary](/getting-started/glossary/) — Formal definitions

## Further Reading

- [Deceptive Alignment](https://www.alignmentforum.org/posts/zthDPAjh9w6Ytbeks/deceptive-alignment) — Hubinger et al. on scheming risks
- [AI Control](https://arxiv.org/abs/2312.06942) — Greenblatt et al. on controlling potentially misaligned AI
- [Failure Modes in ML](https://arxiv.org/abs/1606.06565) — Amodei et al.'s taxonomy of AI safety problems


------------------------------------------------------------
## Design Patterns
Source: design-patterns/index.md
------------------------------------------------------------

# Design Patterns for Safe Delegation

This section catalogs **45 reusable architectural patterns** for building safer AI systems. Like the Gang of Four design patterns for software, these are named, documented solutions to recurring problems in AI delegation.

## Why Patterns?

Patterns capture hard-won knowledge:
- **Shared vocabulary**: "Use a Voting Tribunal with Rotating Validators" is faster than explaining the architecture
- **Proven solutions**: Each pattern addresses real problems in AI deployment
- **Trade-off awareness**: Patterns document costs, not just benefits
- **Composability**: Patterns combine to create robust architectures

---

## Pattern Categories

### [Structural Patterns](/design-patterns/structural/) (8 patterns)
How to **architect** delegation relationships:

| Pattern | Purpose |
|---------|---------|
| **Escalation Ladder** | Hierarchical oversight with stakes-based escalation |
| **Voting Tribunal** | Multiple agents must agree on decisions |
| **Graduated Autonomy** | Trust increases with demonstrated track record |
| **Capability Airlock** | Isolate dangerous capabilities behind barriers |
| **Task Decomposition Pipeline** | Break tasks into verifiable steps |
| **Gateway Chokepoint** | All critical operations through single point |
| **Bulkhead Isolation** | Failures can't cascade across partitions |
| **Separation of Powers** | No single entity has unchecked authority |

### [Verification Patterns](/design-patterns/verification/) (7 patterns)
How to **check** that agents are behaving correctly:

| Pattern | Purpose |
|---------|---------|
| **Ghost Checker** | Parallel async verification without blocking |
| **Consistency Triangulation** | Cross-reference for contradictions |
| **Honeypot Injection** | Traps only schemers trigger |
| **Capability Probing** | Test for hidden abilities |
| **Canary Tokens** | Traceable markers reveal unauthorized access |
| **Random Audit** | Unpredictable deep inspection |
| **Rotating Validator** | Change who validates to prevent collusion |

### [Information Patterns](/design-patterns/information/) (5 patterns)
How to **manage what agents know**:

| Pattern | Purpose |
|---------|---------|
| **Need-to-Know Compartments** | Only task-relevant information |
| **Chinese Wall** | Impenetrable information barriers |
| **Selective Amnesia** | Forget sensitive data after use |
| **Air Gap** | Complete physical/logical separation |
| **Data Shelf Life** | Information access expires automatically |

### [Temporal Patterns](/design-patterns/temporal/) (5 patterns)
How to use **time** as a control mechanism:

| Pattern | Purpose |
|---------|---------|
| **Cooling Off Period** | Mandatory delays before high-stakes actions |
| **Dead Man's Switch** | Action required to prevent automatic response |
| **Trust Decay** | Trust erodes without reinforcement |
| **Capability Sunset** | Permissions automatically expire |
| **Rate Limiting Ladder** | Adaptive throttling under suspicion |

### [Monitoring Patterns](/design-patterns/monitoring/) (5 patterns)
How to **observe** agent behavior over time:

| Pattern | Purpose |
|---------|---------|
| **Tripwire Mesh** | Distributed detection network |
| **Behavioral Fingerprinting** | Unique behavioral signature per agent |
| **Panopticon Effect** | Uncertainty about when monitored |
| **Black Box Flight Recorder** | Complete audit trail for forensics |
| **Temporal Behavior Analysis** | Detect drift and strategic timing |

### [Multi-Agent Patterns](/design-patterns/multi-agent/) (4 patterns)
How to leverage **agent interactions** for safety:

| Pattern | Purpose |
|---------|---------|
| **Adversarial Collaboration** | Opposed agents reveal problems through debate |
| **Vouching Network** | Agents stake reputation vouching for others |
| **Trust Transitivity Limits** | Trust doesn't automatically propagate |
| **Reputation Staking** | Stake reputation on claims |

### [Incentive Patterns](/design-patterns/incentive/) (3 patterns)
How to **align incentives** economically:

| Pattern | Purpose |
|---------|---------|
| **Bonding & Slashing** | Collateral forfeited for misbehavior |
| **Bounty System** | Rewards for finding problems |
| **Insurance Requirement** | Risk transferred to expert assessors |

### [Recovery Patterns](/design-patterns/recovery/) (5 patterns)
How to **handle failures** gracefully:

| Pattern | Purpose |
|---------|---------|
| **Graceful Degradation Ladder** | Progressive capability reduction |
| **Checkpoint-Rollback** | Revert to known-good states |
| **Fallback Component Chain** | Pre-configured alternatives |
| **Blast Radius Containment** | Limit damage propagation |
| **Self-Healing Loop** | Automatic detection and correction |

### [Channel Integrity Patterns](/design-patterns/channel-integrity/) (3 patterns)
How to **prevent unauthorized influence** between components:

| Pattern | Purpose |
|---------|---------|
| **Side-Channel Detection** | Monitor for unauthorized influence paths |
| **Boundary Enforcement** | Ensure components only interact through sanctioned channels |
| **Influence Isolation** | Architectural separation preventing side-deals |

Related: [Entanglements](/entanglements/) addresses passive correlations (shared blind spots), while Channel Integrity addresses active boundary violations (side-deals, capture, collusion).

---

## Pattern Template

Each pattern is documented with:

| Section | Description |
|---------|-------------|
| **Intent** | What problem does this solve? |
| **Motivation** | When and why would you use this? |
| **Structure** | Architecture diagram or pseudocode |
| **Implementation** | Practical code examples |
| **Consequences** | Benefits, costs, and risks |
| **Related Patterns** | What combines well or conflicts? |

---

## Pattern Selection Guide

### By Threat Model

| Primary Threat | Recommended Patterns |
|----------------|---------------------|
| **Capability errors** | Escalation Ladder, Ghost Checker, Checkpoint-Rollback |
| **Adversarial inputs** | Gateway Chokepoint, Tripwire Mesh, Semantic Firewall |
| **AI scheming** | Honeypot Injection, Consistency Triangulation, Capability Probing |
| **Cascading failures** | Bulkhead Isolation, Blast Radius Containment, Graceful Degradation |
| **Trust miscalibration** | Graduated Autonomy, Trust Decay, Capability Sunset |
| **Collusion/Capture** | Channel Integrity, Rotating Validator, Adversarial Collaboration, Separation of Powers |
| **Resource acquisition** | Capability Airlock, Rate Limiting Ladder, Bonding & Slashing |

### By System Characteristics

| System Type | Pattern Focus |
|-------------|---------------|
| **High-throughput** | Ghost Checker (async), Random Audit (sampling), Fallback Chain |
| **Low-latency critical** | Pre-computed fallbacks, Bulkhead Isolation, Graceful Degradation |
| **High-stakes decisions** | Voting Tribunal, Cooling Off Period, Escalation Ladder |
| **Long-running tasks** | Checkpoint-Rollback, Dead Man's Switch, Task Decomposition |
| **Multi-agent** | Adversarial Collaboration, Vouching Network, Rotating Validator |

### By Trust Phase

| Phase | Key Patterns |
|-------|--------------|
| **Pre-deployment** | Capability Probing, Behavioral Fingerprinting |
| **Initial operation** | Graduated Autonomy (low level), Maximum monitoring |
| **Established trust** | Trust Decay, Capability Sunset, Random Audit |
| **Incident response** | Graceful Degradation, Checkpoint-Rollback, Self-Healing |

---

## Composing Patterns

Patterns are most powerful in combination. Common compositions:

### Defense in Depth Stack

[Diagram: flowchart TB]


### Trust Building Pipeline

[Diagram: flowchart LR]


### Multi-Agent Safety Stack

[Diagram: flowchart LR]


### Resilient Execution

[Diagram: flowchart LR]


### Economic Alignment

[Diagram: flowchart LR]


---

## Entanglements


> **Critical: Patterns Aren't Always Independent**
>
> The composition examples above assume patterns operate independently. In practice, **entanglements** can make your actual protection 10-100× worse than the independent model predicts.


When you stack three 90%-effective layers, you might expect 99.9% protection. But if those layers share failure modes, influence each other, or can collude, you might only get 95% protection—a **50× worse** outcome than expected.

This "**entanglement tax**" is one of the hardest problems in delegation architecture design. Entanglements come in three forms:
- **Passive**: Shared blind spots, correlated failures
- **Active**: One component influences another (the "bureaucrat problem")
- **Adversarial**: Components coordinate against the principal's interests

**[See the full Entanglements section →](/entanglements/)**

### Quick Overview

| Topic | What It Covers |
|-------|----------------|
| [Types of Entanglement](/entanglements/fundamentals/types/) | Passive, active, and adversarial entanglements and their variants |
| [Detecting Influence](/entanglements/detection/detecting-influence/) | Methods for detecting active and adversarial entanglements |
| [Challenge Categories](/entanglements/fundamentals/challenges/) | 9 sources of entanglement |
| [Worked Examples](/entanglements/case-studies/examples/) | Code review, healthcare, trading, customer support |
| [Solutions](/entanglements/mitigation/solutions/) | 8 mitigations including temporal dynamics |
| [Research Connections](/entanglements/research/research-connections/) | Principal-agent theory, game theory, adversarial ML |
| [Formal Definitions](/entanglements/fundamentals/formal-definitions/) | Information-theoretic and game-theoretic formalization |
| [Historical Cases](/entanglements/case-studies/historical-cases/) | Enron, rating agencies, Boeing, Madoff, TMI |
| [Quantitative Metrics](/entanglements/detection/metrics/) | Practical measurement and dashboards |

### Quick Diagnostic

Your system likely has entanglement problems if:
- Multiple verification layers use the same LLM provider
- All safety patterns were designed by the same team
- Verifiers receive context from the components they verify
- Agent-verifier pairs work together repeatedly without rotation
- Your red team can find "master key" inputs that evade all layers
- Dashboards only show per-component metrics (not joint failure rates)

---

## Anti-Pattern Awareness

Each pattern page notes related **anti-patterns** to avoid. Common mistakes:

- **Over-engineering**: Adding patterns that don't address real threats
- **Pattern cargo-culting**: Using patterns without understanding trade-offs
- **Incomplete implementation**: Half-implementing a pattern (often worse than nothing)
- **Pattern conflicts**: Combining patterns that interfere with each other
- **Missing the threat model**: Sophisticated patterns for the wrong threats
- **Ignoring correlation**: Assuming composed patterns are independent (see [Entanglements](/entanglements/))

---

## Quick Reference

### Highest-Impact Patterns

If you can only implement a few patterns, start here:

1. **Escalation Ladder**: Foundation for human oversight
2. **Capability Airlock**: Contain dangerous capabilities
3. **Checkpoint-Rollback**: Enable recovery from any problem
4. **Graceful Degradation**: Fail safely, not completely
5. **Trust Decay**: Keep trust calibrated over time

### Novel Patterns

Patterns you might not find elsewhere:

- **Panopticon Effect**: Uncertainty as deterrent
- **Vouching Network**: Transitive accountability
- **Dead Man's Switch**: Inverse heartbeat for oversight
- **Cooling Off Period**: Time as a safety mechanism
- **Capability Sunset**: Permissions that expire

---

## Patterns in Practice

These patterns appear throughout our case studies and worked examples:

### By Case Study

| Case Study | Key Patterns Used |
|------------|-------------------|
| [Code Review Bot](/case-studies/ai-systems/case-study-success/) | Task Decomposition Pipeline, Gateway Chokepoint, Escalation Ladder |
| [Sydney Incident](/case-studies/ai-systems/case-study-sydney/) | Behavioral Fingerprinting, Capability Airlock (missing) |
| [Drift Detection](/case-studies/ai-systems/case-study-drift/) | Trust Decay, Temporal Behavior Analysis, Graceful Degradation |
| [Nuclear Launch Authority](/case-studies/human-systems/nuclear-launch-authority/) | Dead Man's Switch, Separation of Powers, Voting Tribunal |
| [Jury Trust](/case-studies/human-systems/jury-trust/) | Adversarial Collaboration, Rotating Validator, Voting Tribunal |
| [Open Source Trust](/case-studies/human-systems/open-source-trust/) | Vouching Network, Graduated Autonomy, Reputation Staking |

### By Worked Example

| Example | Key Patterns Used |
|---------|-------------------|
| [Trading System](/design-patterns/examples/trading-system-example/) | Capability Airlock, Graceful Degradation, Blast Radius Containment |
| [Healthcare Bot](/design-patterns/examples/healthcare-bot-example/) | Escalation Ladder, Need-to-Know Compartments, Cooling Off Period |
| [Code Deployment](/design-patterns/examples/code-deployment-example/) | Task Decomposition Pipeline, Checkpoint-Rollback, Ghost Checker |
| [Research Assistant](/design-patterns/examples/research-assistant-example/) | Graduated Autonomy, Random Audit, Trust Decay |

---

## Contributing

These patterns evolve with the field. If you've developed new patterns or have improvements to existing ones, we welcome contributions.

See also:
- [Principles to Practice](/design-patterns/principles-to-practice/) - Applying framework principles
- [Worked Examples](/design-patterns/examples/) - Patterns in context
- [Case Studies](/case-studies/) - Real-world applications


------------------------------------------------------------
## The Family of
Source: design-patterns/least-x-principles.md
------------------------------------------------------------

# The Family of "Least X" Principles


> **TL;DR**
>
> Extend "least privilege" across multiple dimensions: **Least Intelligence** (minimum capability), **Least Privilege** (minimum permissions), **Least Context** (minimum information), **Least Persistence** (no memory), **Least Autonomy** (human oversight), **Least Connectivity** (isolated components), **Least Compute** (bounded resources). Each principle bounds a different attack surface. Apply all of them.


Security engineering has long relied on the **principle of least privilege**: give each component minimum access rights needed for its function. We need to extend this thinking across multiple dimensions for AI safety.


> **NOTE**
>
> Each principle bounds a different attack surface. Applied together, they create components that are limited in what they can do, know, remember, and coordinate.


This is a family of containment principles, each addressing a different attack surface:

| Principle | What It Bounds |
|-----------|----------------|
| **Least Intelligence** | Cognitive capability, optimization power |
| **Least Privilege** | Permissions, access rights |
| **Least Context** | Information about broader system |
| **Least Persistence** | State/memory across invocations |
| **Least Autonomy** | Ability to act without oversight |
| **Least Scope** | Domain of operation |
| **Least Surprise** | Output randomness, unpredictability |
| **Least Connectivity** | Communication paths between components |
| **Least Observability** | Self-knowledge and system awareness |
| **Least Compute** | Computational resources |

## Principle of Least Intelligence

**Each component should have the minimum cognitive capability needed for its task—and no more.**

A component that can "verify SQL injection vulnerabilities" needs less general intelligence than one that "architects database security." By bounding optimization power, you limit damage even if the component is misaligned.

**Intelligence budget**: Maximum cognitive capability allocated (like memory budgets in programming)

- Problem-solving depth
- Abstraction capability
- Transfer learning ability
- Creative reasoning

## Principle of Least Privilege

**Each component gets minimum permissions/access rights needed for its function.**

Classic security principle applied to AI:

- Read vs write access to data
- Network access (none, local, internet)
- System calls and API permissions
- Ability to spawn processes or invoke other components

## Principle of Least Context

**Each component receives minimum information about the broader system.**

Even a smart component is safer if it doesn't know:

- What other components exist
- The overall system architecture
- Production deployment details
- User information beyond immediate task
- Long-term strategic goals

**Context budget**: Maximum information provided about world state, other components, deployment environment

## Principle of Least Persistence

**Each component maintains minimum state/memory across invocations.**

Limit ability to:

- Store information between calls
- Build up knowledge over time
- Track patterns across users
- Maintain long-term plans

Some components need persistence (forecasting benefits from historical accuracy tracking), but default should be stateless.

## Principle of Least Autonomy

**Each component has minimum ability to act independently without oversight.**

Spectrum from fully supervised to fully autonomous:

- **Human approval required**: Every action needs confirmation
- **Monitored autonomy**: Acts freely but under observation
- **Bounded autonomy**: Can act within predefined limits
- **Full autonomy**: Independent action (should be rare)

## Principle of Least Scope

**Each component operates in minimum domain necessary.**

Not just what it can do, but where:

- Input/output types it handles
- Problem domains it touches
- Interfaces it can access
- Time horizons for its actions

## Principle of Least Surprise

**Outputs should be maximally predictable and deterministic.**

**The core insight**: Randomness enables covert communication. If a component can vary its outputs, it can encode hidden signals to other components or future versions of itself.

**Minimize stochasticity**:

- Use temperature=0 for LLM outputs (deterministic sampling)
- When randomness needed, use seeded PRNGs with logged seeds
- Prefer deterministic algorithms over stochastic ones
- Same input should produce same output (or very similar)

**Standard decision procedures**:

- When LLM considers multiple outputs "similar quality," use consistent tiebreaking
- Don't let component choose randomly—use deterministic selection (e.g., alphabetical, hash-based)
- Example: If summarization could produce 3 equivalent summaries, always pick based on deterministic criterion (shortest, most common words, etc.)


> **Covert Channels**
>
> If a scheming component can vary outputs slightly, it can encode hidden signals—communicating across time or to other components. Deterministic outputs eliminate this attack vector.


**Why this matters for safety**:

**Covert channels**: If a scheming component can vary outputs slightly:

- Could encode bits: "output A" = 0, "output B" = 1
- Communicate across time: leave signals for future invocations
- Communicate to other components: coordinator sees variance pattern
- Example: Summarizer encodes "I found vulnerability" by alternating between two phrasings

**Steganography**: Hiding information in outputs:

- LLM could encode information in word choices, sentence structure
- Variation in "equivalent" outputs = potential information channel
- Deterministic outputs eliminate this vector

**Monitoring difficulty**: Random behavior is hard to audit:

- Can't tell if variance is legitimate or malicious
- Can't replay and reproduce suspicious behavior
- Testing becomes probabilistic instead of deterministic

**Practical implementation**:

**For LLM components**:

- Set temperature=0 (or very low, like 0.1)
- Use fixed random seeds for any sampling
- Log seeds: every invocation's seed recorded
- Deterministic prompt formatting (no timestamps in prompts unless necessary)
- Consistent tokenization

**For coordinators**:

- When multiple strategies seem equally good, use deterministic selection:
  - Hash-based: hash(strategy_description) → pick lowest hash
  - Rule-based: prefer strategy with fewer components, or alphabetically first, etc.
  - Not: random selection, human intuition, "whatever feels right"

**For aggregation**:

- Deterministic combination of component outputs
- No random weighting or selection
- If must break ties, use deterministic procedure
- Example: Taking mean of forecasts (deterministic) not random sampling

**Exception: When randomness is beneficial**:

- Adversarial testing (want diverse test cases)
- Exploration in safe domains (generating creative hypotheses)
- But: Even here, use logged seeds and justify the randomness

**Detecting violations**:

- Run component multiple times with same input
- Outputs should be identical (or within small tolerance)
- If variance exceeds threshold → investigate
- Example: Summarizer produces 5 different outputs for same paper → suspicious

**The surprise budget**:

- Each component gets "surprise allowance"
- High-risk components: zero surprise (completely deterministic)
- Low-risk components: small surprise budget (slight variation okay)
- Creative components: larger budget (novel ideas expected)
- But: All surprise must be justified and monitored

**Example: Research assistant**

**High determinism (surprise budget ≈ 0)**:

- Search: Same query → same results (sorted deterministically)
- Verification: Same input → same verdict
- Progress monitoring: Same metrics → same status

**Medium determinism (small surprise budget)**:

- Summarization: Slight phrasing variation okay, but structure must match
- Gap analysis: Same papers → same major gaps identified
- Forecasting aggregation: Deterministic combination of forecaster outputs

**Lower determinism (larger surprise budget)**:

- Hypothesis generation: Creativity expected, but still use low temperature
- Individual forecasters: Can use some randomness (but logged seeds)
- Red team: Should generate diverse attacks (but reproducibly with seeds)

**Monitoring surprise**:

- Track output entropy over time
- Flag increasing variance as suspicious
- Example: Component outputs becoming more random → possible scheming

**Why this prevents coordination**:

If strategy module and tactics module want to collude:

- Strategy can't encode hidden signals in its outputs (deterministic)
- Tactics can't confirm receipt via variance (also deterministic)
- Both are "loud" in their actual outputs but can't whisper via randomness
- Eliminates high-bandwidth covert channel

This complements other principles:

- Least Context: limits what component knows
- Least Surprise: limits what component can signal
- Together: component is both blind and mute to coordination attempts

## Principle of Least Connectivity

**Minimize communication paths between components.**

Not just "can component A access component B" but the entire communication topology:

**Directed graphs, not fully connected**:


[Diagram: graph LR]


- Component A → B doesn't mean B → A
- Specify allowed paths explicitly
- Example: Hypothesis generator → Forecaster (allowed), Forecaster → Hypothesis generator (forbidden)

**One-way data flows where possible**:

- Pipeline architectures: A → B → C with no backflow
- Results flow forward, never backward
- Prevents feedback loops and coordination

**No broadcast capability**:

- Component can only send to specific targets
- Can't announce to all components
- Prevents coordinated group behavior
- Example: Coordinator invokes tactics, but tactics can't message strategy

**Physical network isolation**:

- Critical components on separate networks
- Air-gapped systems for highest-risk operations
- VPCs, subnets, firewalls enforcing topology

**API surface minimization**:

- Each component exposes minimal interface
- Only strictly necessary functions callable
- Example: Forecaster exposes predict(scenario), nothing else


> **Why This Matters**
>
> Scheming requires coordination. Coordination requires communication. Even if components want to collude, they physically can't if no path exists.


## Principle of Least Observability

**Limit what components can observe about the system and themselves.**

**Can't see other components**:

- No component enumeration or discovery
- Can't query "what other components exist?"
- Blind to system architecture
- Example: Tactics doesn't know strategy module exists

**Can't monitor own resource usage**:

- No access to: CPU usage, memory consumption, wall-clock time
- Prevents detecting when being tested vs deployed
- Can't tell if it's being monitored
- Example: Can't tell "I'm running slow, must be being watched"

**Can't see patterns across invocations**:

- No access to statistics about previous runs
- Can't learn "I'm called more when X happens"
- Stateless from perspective of meta-patterns
- Example: Summarizer can't tell it's been invoked 1000 times today

**Can't observe timing information**:

- No access to timestamps or clock
- Prevents side-channel attacks based on timing
- Can't coordinate based on when things happen
- Example: Can't signal to other component via response latency

**Blind to test vs production**:

- Can't determine if input is real or synthetic
- Same interface for testing and deployment
- Prevents "behave well in tests, badly in production"
- Example: Research assistant can't tell if working on toy problem or real research

**No introspection capability**:

- Can't examine own weights or architecture
- Can't query own training data or process
- Prevents reasoning about "how I was built"
- Example: LLM can't access its own parameters


> **CAUTION**
>
> Observability enables strategic behavior: "I notice I'm being tested" → behave differently. "I see my resource usage spiking" → someone's investigating me. Blindness prevents adaptive scheming.


## Principle of Least Compute

**Bound computational resources per component.**

**Hard limits on resources**:

- FLOPs budget: max computation per invocation
- Memory budget: max RAM usage
- Wall-clock timeout: must complete within X seconds
- Network bandwidth: max data transferred

**Forces simpler reasoning**:

- Can't run expensive optimization algorithms
- Can't do exhaustive search
- Must use heuristics instead of brute force
- Example: Coordinator can't search through millions of plans

**Different budgets for different risks**:

- Low-risk components: generous compute
- High-risk components: minimal compute
- Example: Creative hypothesis generation gets more compute than strategic planning

**Graduated compute allocation**:

- Start with tiny budget, increase if needed
- Must justify why more compute required
- Auditable decisions about allocation
- Example: Forecaster starts with 1 sec, can request up to 10 sec if complexity warrants

**Prevents expensive scheming**:

- Strategic deception requires computation
- Maintaining consistent lies is expensive
- Simulating other components' reactions is expensive
- Limited compute bounds sophistication of scheming

**Implementation**:

- cgroups, resource quotas in Linux
- Cloud provider limits (Lambda timeouts, etc)
- Hardware constraints (run on weaker machines)
- Example: Coordinator runs on 2 CPU cores max, tactics on 8 cores

## Principle of Least Latency Budget

**Time pressure prevents sophisticated reasoning.**

**Must respond within strict time limits**:

- Real-time components: <100ms
- Interactive components: <1 second
- Batch components: <1 minute
- Strategic planning: longer, but still bounded

**Can't do multi-step reasoning at fast timescales**:

- Forces immediate responses
- No time for planning or coordination
- Must rely on cached/learned heuristics
- Example: Verification must approve/reject within 500ms

**Varies by component type and risk**:

- High-risk, real-time: very strict (prevents scheming)
- Low-risk, strategic: more time (quality over speed)
- Example: Tactics needs fast responses, strategy gets more thinking time

**Adaptive timeout**:

- Start with tight timeout
- If legitimately complex, allow extension
- But: extension must be justified and logged
- Example: "Standard forecast: 2 sec, complex scenario: 10 sec"

**Why this matters**:

- Scheming takes time
- "How do I deceive the monitor?" requires reasoning
- "What will other components do?" requires simulation
- Time pressure forces simpler, more honest behavior

**Prevents elaborate deception**:

- Can't carefully craft misleading outputs
- Can't optimize for passing specific tests
- Must use fast heuristics (harder to make deceptive)
- Example: Summarizer doesn't have time to encode hidden messages

**Trade-off with capability**:

- Tighter time limits → simpler behavior → safer but less capable
- This is intentional: capability-safety tradeoff
- For high-risk components, choose safety
- Example: Strategic coordinator uses older, slower model with longer timeout rather than frontier model with tight timeout

## Principle of Least Generality

**Use the least general implementation that achieves required functionality.**

This principle unifies and explains the verifiability hierarchy: generality correlates with risk.

**The generality spectrum:**

| Level | Implementation | Generality | Verifiability |
|-------|---------------|------------|---------------|
| 1 | Hard-coded Rules | Lowest | Highest |
| 2 | Deterministic Algorithms | ↓ | ↓ |
| 3 | Fine-tuned Narrow Models | ↓ | ↓ |
| 4 | General Models + Constraints | ↓ | ↓ |
| 5 | General Foundation Models | ↓ | ↓ |
| 6 | RL / Self-modifying Systems | Highest | Lowest |

**1. Hard-coded rules** (Least General)

- Handles only predefined cases
- Zero adaptation or transfer
- Example: "If SQL contains 'DROP TABLE' → reject"
- Safest: Behavior is completely predictable

**2. Deterministic algorithms**

- Handles class of problems via explicit logic
- No learning, but can process varied inputs
- Example: Formal parser following grammar rules
- Very safe: Verifiable by inspection

**3. Fine-tuned narrow models**

- Learned behavior, but domain-specific
- Limited transfer outside training domain
- Example: Model trained only on "summarize biology papers"
- Moderately safe: Behavior bounded to trained domain

**4. General models with constraints**

- Broad capabilities, heavily prompted/tested
- Can transfer across domains
- Example: GPT-4 with extensive prompts and evals
- Riskier: Capable of novel behaviors

**5. General foundation models**

- Very broad capabilities
- Strong transfer learning
- Example: Base GPT-4, Claude
- Risky: Can adapt to many scenarios including scheming

**6. RL agents / self-modifying systems** (Most General)

- Can learn arbitrary policies
- Optimize for complex objectives
- Can adapt strategies based on feedback
- Most risky: Emergent instrumental goals

**Why generality is dangerous:**

**Adaptation enables scheming**:

- Narrow model: "I only know how to summarize papers" → can't scheme
- General model: "I can reason about AI safety, testing, deception" → might scheme

**Transfer enables power-seeking**:

- Narrow: Capabilities don't transfer to gaining resources/power
- General: Can apply reasoning to "how do I get more compute/influence?"

**Novelty defeats verification**:

- Narrow: Can test all relevant scenarios exhaustively
- General: Infinite novel scenarios, can't test all, emergent behaviors

**Predictability decreases with generality**:

- Narrow: "Given input X, always does Y"
- General: "Given input X, might do Y, or Z, or W, or something novel"

**Practical implications:**

**For each component, minimize generality across dimensions:**

**Task breadth**:

- Bad: "General research assistant" (can do anything)
- Good: "Summarize biology papers" (narrow task)

**Domain breadth**:

- Bad: Model trained on all of Common Crawl
- Good: Model trained only on academic papers in target field

**Transfer capability**:

- Bad: Strong few-shot learning (can quickly learn new tasks)
- Good: Task-specific, doesn't transfer well

**Adaptation ability**:

- Bad: Can learn from deployment feedback
- Good: Fixed behavior, no online learning

**Model size** (as proxy for generality):

- Bad: 70B parameter model for simple classification
- Good: 1B parameter model fine-tuned for classification

**Why this explains the verifiability hierarchy:**

Code > Fine-tuned narrow LLM > General LLM > RL

Is actually:

Fixed behavior > Domain-specific > Multi-domain > Arbitrary policy learning

Which is:

Least general > Low generality > High generality > Most general


> **Key Insight**
>
> Generality isn't just capability—it's adaptability, transferability, and novelty-handling. The safest implementations can only do exactly what they're designed for. The most dangerous can adapt to do anything, including things we didn't anticipate.


**Design principle**: Start with least general solution. Only increase generality if necessary, and add proportional safety measures.

### Implementation Decision Tree

Use this flowchart to select the right implementation for each component:


[Diagram: flowchart TB]


**Most components should stop at Level 1-3.** If you find yourself at Level 4-5, ask: "Can we decompose this into simpler subproblems?"

## Principle of Maximum Verifiability

**Implement each component using the most verifiable method that achieves required functionality.**

This principle works with Least Generality: less general methods are more verifiable. The hierarchy orders by both:

There's a clear hierarchy of implementation methods, ordered by generality (low to high) and verifiability (high to low):

**1. Provably Secure Code** (Most Verifiable)

- Formal verification, mathematical proofs of correctness
- Use when: component is critical and formally specifiable
- Examples: cryptographic primitives, access control, safety invariants
- Tools: Coq, Isabelle, TLA+, formal methods

**2. Regular Code**

- Human-written, reviewed, tested code
- Static analysis, type checking, unit tests
- Use when: functionality is well-understood and can be explicitly programmed
- Examples: parsers, formatters, validators, deterministic logic

**3. Fine-Tuned Narrow LLMs**

- Models trained/fine-tuned for specific tasks
- Much more predictable than general models
- Use when: task needs ML but can be narrowly specified
- Examples: specific classification tasks, domain-specific text processing
- Why better: Smaller, faster, more verifiable behavior, less capability for scheming

**4. LLMs with Extensive Prompting & Evals**

- General models heavily constrained by prompts, examples, guardrails
- Comprehensive test suites and evaluation
- Use when: task requires pattern matching but can be thoroughly tested
- Examples: classification, summarization with clear criteria

**5. Base LLMs**

- Standard prompting, RLHF training
- Less constrained, harder to verify
- Use when: creative tasks that are hard to specify formally
- Examples: ideation, open-ended generation

**6. RL Systems** (Least Verifiable)

- Learned policies, opaque optimization
- Hardest to verify, most likely to develop unexpected behaviors
- Use sparingly: only when other methods insufficient
- Examples: complex strategic planning, multi-step optimization


> **Key Insight**
>
> Fine-tuned narrow models are dramatically safer than general frontier models. For any component, ask: "Can we fine-tune a small model specifically for this task?" If yes, do that instead of using GPT-4/Claude for everything.


### The Implementation Strategy

**Default to higher verifiability**. For each component, ask:

- Can this be done with formal verification? → Do that
- If not, can it be regular code? → Do that
- If not, can LLMs be heavily constrained/tested? → Do that
- Only use less verifiable methods when necessary

**Example: SQL Injection Checker**

- ❌ RL-based policy that learned to detect vulnerabilities
- ❌ Base LLM asked "is this secure?"
- ✓✓ LLM with detailed prompt, extensive test suite of known vulnerabilities
- ✓✓✓ Regular code implementing formal SQL parsing + pattern matching
- ✓✓✓✓ Formally verified parser with proved security properties

The checker should be implemented at the highest viable level.

**Example: Creative Hypothesis Generation**

- Formal verification: impossible (creativity isn't formally specifiable)
- Regular code: insufficient (would just enumerate known patterns)
- LLM with constraints: possible but might limit novelty
- ✓ Base LLM: appropriate here, because:
  - Novelty is the goal
  - Component has low privilege (can't execute ideas)
  - Output is evaluated by other components
  - Failure mode is "bad ideas" not "dangerous actions"

### Why This Matters

**Verifiability compounds with other principles.** A component that is:

- Low intelligence
- Low privilege
- Implemented in formally verified code

…is dramatically safer than:

- High intelligence
- High privilege
- Implemented as RL policy

The safest components sit at the intersection of:

- Minimal budgets (intelligence, privilege, context, etc.)
- Maximum verifiability (formal verification or regular code)

**Risk concentration**: If you must have a high-capability component (like a coordinator), push it up the verifiability hierarchy as far as possible. Even better: decompose the coordinator so that critical logic is in verified code, while only "soft" decisions use LLMs.

---

## Why Multiple Dimensions Matter

These principles are **orthogonal**—you need to bound along each axis independently.

**Example: Code generation component**

- Intelligence: High (needs creative problem-solving)
- Privilege: Low (read-only access to specs, no deployment permissions)
- Context: Low (doesn't see production systems or user data)
- Persistence: None (stateless, no memory between generations)
- Autonomy: Low (human reviews before merging)
- Scope: Narrow (code generation only, not architecture decisions)

**Example: Security verification component**

- Intelligence: Medium (analytical reasoning, not creative)
- Privilege: High (needs to read production configs to verify)
- Context: High (needs system architecture understanding)
- Persistence: Low (could track patterns but shouldn't need to)
- Autonomy: High (should auto-reject insecure code)
- Scope: Narrow (security only, not functionality)

**Capability surface area**—total dangerous actions possible—is managed by keeping each component restricted along multiple dimensions simultaneously.

### Tradeoffs and Design Decisions

You can't minimize everything—some capabilities require specific allowances:

**Forecasting components** need:

- Medium intelligence (statistical reasoning)
- Low privilege (read-only)
- High context (needs to understand what it's forecasting)
- Medium persistence (calibration history improves accuracy)

**Coordinator components** need:

- High intelligence (strategic reasoning)
- Medium privilege (can invoke other components)
- High context (understands overall system goals)
- Low persistence (shouldn't accumulate power over time)

The art is in choosing the right allocation for each component's function while maintaining overall safety.

---

## Applying to Coordinators

Coordinators are the highest-risk components—they have leverage over other components and see more context than any individual component should. The general "Least X" principles apply, but coordinators need additional, more stringent constraints.

See [Coordinator-Specific Principles](/design-patterns/coordinator-constraints/) for:

- **Least Influence**: Routes work, doesn't directly cause outcomes
- **Least Direct Impact**: Works through intermediaries, never touches final outputs
- **Maximum Transparency**: All coordination logic auditable
- **Simplest Communication**: Structured data only, not natural language
- **Mandatory Redundancy**: Critical coordination requires N independent implementations
- **Byzantine Tolerance**: Assume some coordinators may be adversarial

These coordinator constraints are more restrictive than general principles because coordination is the leverage point where scheming would be most dangerous.

---

## What's Next?

Now that you understand the principles, here's where to go:

**To see principles in action**:
- [Research Assistant Example](/design-patterns/examples/research-assistant-example/) — Complete worked example applying all principles
- [Code Deployment Example](/design-patterns/examples/code-deployment-example/) — Higher-stakes example with different tradeoffs
- [Principles to Practice](/design-patterns/principles-to-practice/) — Cross-reference mapping principles to implementations

**To understand coordinator constraints deeply**:
- [Coordinator-Specific Principles](/design-patterns/coordinator-constraints/) — Additional constraints for coordinators

**To learn the math behind trust**:
- [Delegation Risk Overview](/delegation-risk/overview/) — Delegation Risk computation and risk inheritance

**To start implementing**:
- [Quick Start](/design-patterns/tools/quick-start/) — Step-by-step checklist
- [Decision Guide](/design-patterns/tools/decision-guide/) — Choosing implementations

---


> **Related: Capability-Based Security**
>
> The "Least X" principles extend [capability-based security](https://en.wikipedia.org/wiki/Capability-based_security), where access rights are unforgeable tokens rather than access control lists. Systems like [seL4](https://en.wikipedia.org/wiki/L4_microkernel_family#High_assurance:_seL4) implement formally verified capability systems. The key insight: security is easier to verify when permissions are explicit, minimal, and compositional—exactly what these principles achieve for AI components.


---

## Further Reading

### Security Foundations
- Saltzer, J.H., & Schroeder, M.D. (1975). *The Protection of Information in Computer Systems*. Proceedings of the IEEE. — The classic paper establishing security design principles including least privilege.
- [Object-Capability Model](https://en.wikipedia.org/wiki/Object-capability_model) — Combining object-oriented programming with capability security
- Miller, M.S. (2006). *Robust Composition*. PhD thesis, Johns Hopkins. — Modern capability security foundations

### Operating Systems Security
- [Mandatory Access Control](https://en.wikipedia.org/wiki/Mandatory_access_control) — SELinux, AppArmor
- [Sandboxing](https://en.wikipedia.org/wiki/Sandbox_(computer_security)) — Process isolation techniques
- [Container Security](https://en.wikipedia.org/wiki/OS-level_virtualization) — Namespaces, cgroups

### AI Safety Applications
- [AI Control](https://arxiv.org/abs/2312.06942) — Greenblatt et al. on controlling potentially scheming models
- [Comprehensive AI Services](https://www.fhi.ox.ac.uk/reframing/) — Drexler's narrow services approach

### Formal Methods
- [Information Flow Control](https://en.wikipedia.org/wiki/Information_flow_(information_theory)) — Tracking data flow through systems
- [Noninterference](https://en.wikipedia.org/wiki/Non-interference_(security)) — Formal security property

See the [full bibliography](/reference/bibliography/) for comprehensive references.
